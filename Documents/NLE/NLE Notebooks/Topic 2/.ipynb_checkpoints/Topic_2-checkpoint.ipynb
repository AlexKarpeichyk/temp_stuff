{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 2: Basic Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries \n",
    "Run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'\\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources')\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from collections import defaultdict,Counter\n",
    "from itertools import zip_longest\n",
    "from IPython.display import display\n",
    "from random import seed\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pylab as pylab\n",
    "%matplotlib inline\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'large',\n",
    "         'axes.titlesize':'large',\n",
    "         'xtick.labelsize':'large',\n",
    "         'ytick.labelsize':'large'}\n",
    "pylab.rcParams.update(params)\n",
    "from pylab import rcParams\n",
    "from operator import itemgetter, attrgetter, methodcaller\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "This topic (Topic 2) and Topic 3 concern the task of sentiment analysis. You will be using a corpus of **book reviews** within an **Amazon review corpus**.\n",
    "\n",
    "You be exploring various techniques that can be used to classify the sentiment of Amazon book reviews as either positive or negative. \n",
    "\n",
    "You will be developing your own **Word List** classifiers and then comparing them to the **NLTK Na√Øve Bayes** classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training and testing sets\n",
    "During the next two lab sessions you will be training and testing various document classifiers. It is essential that the data used in the testing phase is not used during the training phase, since this can lead to overestimating performance. \n",
    "\n",
    "We now introduce the `split_data` function (defined in the cell below) which can be used to get separate **training** and **testing** sets.\n",
    "\n",
    "### Exercise\n",
    "Look through the code in the following cell, reading the comments and making sure that you understand each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sussex NLTK root directory is \\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources\n"
     ]
    }
   ],
   "source": [
    "from random import sample # have a look at https://docs.python.org/3/library/random.html to see what random.sample does\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "\n",
    " \n",
    "def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n",
    "    \"\"\"\n",
    "    Given corpus generator and ratio:\n",
    "     - partitions the corpus into training data and test data, where the proportion in train is ratio,\n",
    "\n",
    "    :param data: A corpus generator.\n",
    "    :param ratio: The proportion of training documents (default 0.7)\n",
    "    :return: a pair (tuple) of lists where the first element of the \n",
    "            pair is a list of the training data and the second is a list of the test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = list(data) # data is a generator, so this puts all the generated items in a list\n",
    " \n",
    "    n = len(data)  #Found out number of samples present\n",
    "    \n",
    "    train_indices = sample(range(n), int(n * ratio))          #Randomly select training indices\n",
    "    test_indices = list(set(range(n)) - set(train_indices))   #Randomly select testing indices\n",
    " \n",
    "    train = [data[i] for i in train_indices]           #Use training indices to select data\n",
    "    test = [data[i] for i in test_indices]             #Use testing indices to select data\n",
    " \n",
    "    return (train, test)                       #Return split data\n",
    " \n",
    "#Create an Amazon corpus reader pointing at only book reviews\n",
    "book_reader = AmazonReviewCorpusReader().category(\"book\")\n",
    "\n",
    "#The following two lines use the documents function on the Amazon corpus reader. \n",
    "#This returns a generator over reviews in the corpus. \n",
    "#Each review is an instance of a Python class called AmazonReview. \n",
    "#An AmazonReview object contains all the data about a review.\n",
    "pos_train, pos_test = split_data(book_reader.positive().documents())\n",
    "neg_train, neg_test = split_data(book_reader.negative().documents())\n",
    "\n",
    "#You can also combine the training data\n",
    "train = pos_train + neg_train\n",
    "test = pos_test + neg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Make a copy of the cell above and position it below this cell. Then adapt this code so that it splits the book review corpus in various ways (i.e. different test/training ratios), and by measuring the size of the resulting splits, check that the size of both splits match the specified ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 100 900\n",
      "0.5 500 500\n",
      "0.9 900 100\n",
      "0.34 340 660\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/split_checker\n",
    "from random import sample # have a look at https://docs.python.org/3/library/random.html to see what random.sample does\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "\n",
    " \n",
    "def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n",
    "    \"\"\"\n",
    "    Given corpus generator and ratio:\n",
    "     - partitions the corpus into training data and test data, where the proportion in train is ratio,\n",
    "\n",
    "    :param data: A corpus generator.\n",
    "    :param ratio: The proportion of training documents (default 0.7)\n",
    "    :return: a pair (tuple) of lists where the first element of the \n",
    "            pair is a list of the training data and the second is a list of the test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = list(data) # data is a generator, so this puts all the generated items in a list\n",
    " \n",
    "    n = len(data)  #Found out number of samples present\n",
    "    train_indices = sample(range(n), int(n * ratio))          #Randomly select training indices\n",
    "    test_indices = list(set(range(n)) - set(train_indices))   #Randomly select testing indices\n",
    " \n",
    "    train = [data[i] for i in train_indices]           #Use training indices to select data\n",
    "    test = [data[i] for i in test_indices]             #Use testing indices to select data\n",
    " \n",
    "    return (train, test)                       #Return split data\n",
    " \n",
    "#Create an Amazon corpus reader pointing at only book reviews\n",
    "book_reader = AmazonReviewCorpusReader().category(\"book\")\n",
    "ratios = [0.1,0.5,0.9, 0.34]\n",
    "for ratio in ratios:\n",
    "    pos_train, pos_test = split_data(book_reader.positive().documents(),ratio)\n",
    "    neg_train, neg_test = split_data(book_reader.negative().documents(),ratio)\n",
    "    print(ratio,len(pos_train),len(pos_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating word lists\n",
    "The next section will explain how to use a sentiment classifier that bases its decisions on word lists. The classifier requires a list of words indicating positive sentiment, and a second list of words indicating negative sentiment. Given positive and negative word lists, a document's overall sentiment is determined based on counts of occurrences of words that occur in the two lists. In this section we are concerned with the creation of the word lists. We will be considering both hand-crafted lists and automatically generated lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Create a reasonably long hand-crafted list of words that you think indicate positive sentiment.\n",
    "- Create a reasonably long hand-crafted list of words that indicate negative sentiment.\n",
    "\n",
    "Use the following cells to store these lists in the variables `my_positive_word_list` and `my_negative_word_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_positive_word_list = [\"good\",\"great\",\"lovely\",\"wonderful\",\"marvellous\",\"perfect\",\"spectacular\",\"best\",\"beautiful\",\"cool\"] # put your own list here\n",
    "my_negative_word_list = [\"bad\",\"terrible\",\"awful\",\"disgusting\",\"worst\",\"unpleasant\",\"poor\",\"boring\"] # put your own list here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you should try to derive word lists from the data. One way to do this, is to use the most frequent words in positive reviews as your positive list, and the most frequent words in negative reviews as your negative list. This can be done with the [NLTK <code style=\"background-color: #F5F5F5;\">FreqDist</code>](http://www.nltk.org/api/nltk.html#module-nltk.probability) object. \n",
    "\n",
    "### Exercise\n",
    "Make sure that you understand the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'``': 435,\n",
       "          'By': 11,\n",
       "          'a': 1356,\n",
       "          'hypnotism': 1,\n",
       "          'that': 646,\n",
       "          'defies': 2,\n",
       "          'explanation': 4,\n",
       "          '--': 71,\n",
       "          'at': 166,\n",
       "          'least': 16,\n",
       "          'by': 190,\n",
       "          'non-German': 1,\n",
       "          'Hitler': 10,\n",
       "          'held': 6,\n",
       "          'the': 2683,\n",
       "          'allegiance': 1,\n",
       "          'and': 1552,\n",
       "          'trust': 4,\n",
       "          'of': 1631,\n",
       "          'this': 537,\n",
       "          'remarkable': 11,\n",
       "          'people': 78,\n",
       "          'to': 1343,\n",
       "          'last': 25,\n",
       "          '.': 2459,\n",
       "          'It': 158,\n",
       "          'was': 257,\n",
       "          'inevitable': 2,\n",
       "          'they': 118,\n",
       "          'would': 88,\n",
       "          'follow': 19,\n",
       "          'him': 46,\n",
       "          'blindly': 1,\n",
       "          ',': 2407,\n",
       "          'like': 104,\n",
       "          'dumb': 1,\n",
       "          'cattle': 2,\n",
       "          'but': 210,\n",
       "          'also': 91,\n",
       "          'with': 395,\n",
       "          'touching': 3,\n",
       "          'faith': 9,\n",
       "          'even': 72,\n",
       "          'an': 235,\n",
       "          'enthusiasm': 2,\n",
       "          'raised': 4,\n",
       "          'them': 73,\n",
       "          'above': 9,\n",
       "          'animal': 9,\n",
       "          'herd': 1,\n",
       "          'over': 54,\n",
       "          'precipice': 1,\n",
       "          'destruction': 4,\n",
       "          'nation': 6,\n",
       "          'This': 202,\n",
       "          'book': 667,\n",
       "          'is': 958,\n",
       "          'based': 14,\n",
       "          'principally': 1,\n",
       "          'on': 301,\n",
       "          'captured': 2,\n",
       "          'German': 8,\n",
       "          'documents': 3,\n",
       "          'interrogations': 1,\n",
       "          'testimony': 3,\n",
       "          'Nuremberg': 1,\n",
       "          'trials': 3,\n",
       "          'military': 8,\n",
       "          'officers': 2,\n",
       "          'civilian': 1,\n",
       "          'officials': 2,\n",
       "          'diaries': 1,\n",
       "          'memoirs': 2,\n",
       "          'which': 115,\n",
       "          'some': 116,\n",
       "          'have': 262,\n",
       "          'left': 11,\n",
       "          'Shirer': 3,\n",
       "          \"'s\": 502,\n",
       "          'experience': 31,\n",
       "          'in': 787,\n",
       "          'Third': 2,\n",
       "          'Reich': 3,\n",
       "          'became': 13,\n",
       "          'one': 172,\n",
       "          'most': 75,\n",
       "          'respected': 3,\n",
       "          'journalists': 1,\n",
       "          'wartime': 1,\n",
       "          'Europe': 8,\n",
       "          'The': 334,\n",
       "          'exhaustive': 3,\n",
       "          'detailed': 11,\n",
       "          'extensive': 4,\n",
       "          'notes': 7,\n",
       "          'bibliography': 1,\n",
       "          'A': 49,\n",
       "          'bit': 27,\n",
       "          'patience': 2,\n",
       "          'needed': 12,\n",
       "          'get': 59,\n",
       "          'through': 57,\n",
       "          'dialect': 1,\n",
       "          'often': 19,\n",
       "          'quoting': 1,\n",
       "          'Mein': 2,\n",
       "          'Kampf': 2,\n",
       "          'brings': 7,\n",
       "          'forth': 3,\n",
       "          'wonderful': 21,\n",
       "          'descriptions': 8,\n",
       "          'who': 166,\n",
       "          'played': 4,\n",
       "          'role': 12,\n",
       "          'those': 61,\n",
       "          'troubled': 2,\n",
       "          'times': 19,\n",
       "          'individual': 16,\n",
       "          'battles': 4,\n",
       "          'are': 332,\n",
       "          'not': 266,\n",
       "          'so': 126,\n",
       "          'will': 134,\n",
       "          'need': 35,\n",
       "          'refer': 4,\n",
       "          'other': 83,\n",
       "          'sources': 5,\n",
       "          'for': 511,\n",
       "          'more': 157,\n",
       "          'look': 29,\n",
       "          'these': 57,\n",
       "          'horrors': 1,\n",
       "          'ensued': 1,\n",
       "          'important': 33,\n",
       "          'interested': 17,\n",
       "          'Germany': 4,\n",
       "          'life': 91,\n",
       "          'time': 95,\n",
       "          'rises': 1,\n",
       "          'quick': 10,\n",
       "          'run': 8,\n",
       "          'youth': 1,\n",
       "          'history': 40,\n",
       "          'then': 51,\n",
       "          'politics': 8,\n",
       "          'megalomania': 1,\n",
       "          'led': 8,\n",
       "          'national': 1,\n",
       "          'socialism': 1,\n",
       "          'rise': 6,\n",
       "          'dictatorship': 4,\n",
       "          'Treaty': 2,\n",
       "          'Versais': 1,\n",
       "          'fall': 10,\n",
       "          'mark': 2,\n",
       "          'lead': 9,\n",
       "          'economic': 8,\n",
       "          'allowing': 1,\n",
       "          'welcome': 1,\n",
       "          'new': 60,\n",
       "          'messiah': 1,\n",
       "          'deceit': 2,\n",
       "          'buildup': 1,\n",
       "          'could': 64,\n",
       "          'now': 43,\n",
       "          'begin': 6,\n",
       "          'coups': 1,\n",
       "          'overthrow': 1,\n",
       "          'Nazis': 2,\n",
       "          'had': 84,\n",
       "          'already': 16,\n",
       "          'failed': 2,\n",
       "          'continue': 8,\n",
       "          'fail': 5,\n",
       "          'throughout': 15,\n",
       "          'war': 6,\n",
       "          'For': 34,\n",
       "          'it': 501,\n",
       "          'either': 16,\n",
       "          'death': 11,\n",
       "          'or': 185,\n",
       "          'fighting': 3,\n",
       "          'massacres': 1,\n",
       "          'Russians': 1,\n",
       "          'began': 6,\n",
       "          'soon': 15,\n",
       "          'after': 38,\n",
       "          'invasion': 4,\n",
       "          'East': 15,\n",
       "          'French': 7,\n",
       "          'countries': 4,\n",
       "          'were': 85,\n",
       "          'caught': 3,\n",
       "          'up': 80,\n",
       "          'massacre': 3,\n",
       "          'William': 2,\n",
       "          'gives': 14,\n",
       "          'us': 40,\n",
       "          'eyewitness': 2,\n",
       "          'reports': 2,\n",
       "          'Jewish': 11,\n",
       "          'cleansing': 1,\n",
       "          'started': 7,\n",
       "          'nazis': 1,\n",
       "          'took': 16,\n",
       "          'their': 106,\n",
       "          'first': 82,\n",
       "          'kept': 7,\n",
       "          'developing': 4,\n",
       "          'efficient': 1,\n",
       "          'means': 17,\n",
       "          'extermination': 1,\n",
       "          'experimentation': 5,\n",
       "          'There': 42,\n",
       "          'start': 12,\n",
       "          'W.W.II': 1,\n",
       "          'Poland': 2,\n",
       "          'freighting': 1,\n",
       "          'information': 43,\n",
       "          'discovered': 5,\n",
       "          'what': 128,\n",
       "          'been': 83,\n",
       "          'store': 4,\n",
       "          'Britain': 3,\n",
       "          'if': 85,\n",
       "          'invaded': 1,\n",
       "          'mention': 8,\n",
       "          'world': 62,\n",
       "          'slaughter': 2,\n",
       "          'Russia': 4,\n",
       "          'Nazi': 4,\n",
       "          'pinnacle': 1,\n",
       "          'arrives': 1,\n",
       "          'about': 202,\n",
       "          '2/3': 1,\n",
       "          'way': 75,\n",
       "          'where': 24,\n",
       "          'begins': 10,\n",
       "          'Allied': 1,\n",
       "          'advance': 1,\n",
       "          'action': 13,\n",
       "          'eventually': 5,\n",
       "          'stop': 14,\n",
       "          'New': 17,\n",
       "          'order': 14,\n",
       "          'lack': 7,\n",
       "          'doing': 16,\n",
       "          'anything': 19,\n",
       "          'from': 191,\n",
       "          'walking': 3,\n",
       "          'into': 86,\n",
       "          'Rhineland': 1,\n",
       "          'should': 43,\n",
       "          'be': 231,\n",
       "          'forgotten': 2,\n",
       "          'Does': 3,\n",
       "          'blame': 1,\n",
       "          'partly': 2,\n",
       "          'Chamberlain': 1,\n",
       "          'generals': 1,\n",
       "          '?': 93,\n",
       "          'Maybe': 2,\n",
       "          'read': 222,\n",
       "          'disregarded': 1,\n",
       "          'contents': 2,\n",
       "          'millions': 1,\n",
       "          'lost': 6,\n",
       "          'lives': 21,\n",
       "          'propaganda': 4,\n",
       "          'press': 3,\n",
       "          'indoctrinated': 1,\n",
       "          'many': 108,\n",
       "          'opportunities': 1,\n",
       "          'France': 4,\n",
       "          'early': 14,\n",
       "          'Have': 3,\n",
       "          'we': 73,\n",
       "          'learned': 7,\n",
       "          'past': 24,\n",
       "          'Is': 7,\n",
       "          'there': 84,\n",
       "          'parallel': 1,\n",
       "          'our': 64,\n",
       "          'country': 9,\n",
       "          'Germans': 1,\n",
       "          'imposed': 2,\n",
       "          'tyranny': 1,\n",
       "          'themselves': 14,\n",
       "          'Hegel': 2,\n",
       "          'influences': 3,\n",
       "          'Marx': 15,\n",
       "          'Marxism': 1,\n",
       "          'still': 39,\n",
       "          'very': 104,\n",
       "          'much': 82,\n",
       "          'alive': 9,\n",
       "          'today': 19,\n",
       "          'humanism': 1,\n",
       "          'nationalism': 17,\n",
       "          'supreme': 1,\n",
       "          'state': 33,\n",
       "          'racism': 6,\n",
       "          'mysticism': 1,\n",
       "          'arrived': 2,\n",
       "          'as': 365,\n",
       "          'expected': 7,\n",
       "          'great': 106,\n",
       "          'shape': 2,\n",
       "          'Thanks': 3,\n",
       "          'First': 5,\n",
       "          'few': 39,\n",
       "          'words': 12,\n",
       "          'each': 39,\n",
       "          'four': 15,\n",
       "          'different': 43,\n",
       "          'stories': 27,\n",
       "          'Man': 3,\n",
       "          'Black': 1,\n",
       "          'Suit': 1,\n",
       "          '-': 84,\n",
       "          '3': 9,\n",
       "          'stars': 13,\n",
       "          'story': 106,\n",
       "          '9-year-old': 1,\n",
       "          'boy': 5,\n",
       "          'meets': 3,\n",
       "          'Devil': 4,\n",
       "          'himself': 11,\n",
       "          'scary': 4,\n",
       "          'especially': 28,\n",
       "          'because': 60,\n",
       "          'his': 213,\n",
       "          'older': 7,\n",
       "          'brother': 5,\n",
       "          'died': 4,\n",
       "          'year': 18,\n",
       "          'before': 33,\n",
       "          'tells': 14,\n",
       "          'mother': 19,\n",
       "          'has': 173,\n",
       "          'just': 109,\n",
       "          'same': 31,\n",
       "          'weakest': 1,\n",
       "          'although': 15,\n",
       "          'being': 47,\n",
       "          'does': 72,\n",
       "          \"n't\": 162,\n",
       "          'really': 79,\n",
       "          'come': 32,\n",
       "          'across': 10,\n",
       "          'serious': 9,\n",
       "          'threat': 1,\n",
       "          'What': 18,\n",
       "          'kind': 15,\n",
       "          'second-rate': 1,\n",
       "          'ca': 19,\n",
       "          'catch': 1,\n",
       "          'decides': 4,\n",
       "          'away': 20,\n",
       "          'In': 71,\n",
       "          'Everything': 7,\n",
       "          'Eventual': 4,\n",
       "          'Stephen': 10,\n",
       "          'King': 16,\n",
       "          'writes': 12,\n",
       "          'won': 4,\n",
       "          'prestigious': 2,\n",
       "          'best': 65,\n",
       "          'short': 21,\n",
       "          'award': 2,\n",
       "          '1996': 2,\n",
       "          'surprise': 6,\n",
       "          'That': 22,\n",
       "          'surprises': 1,\n",
       "          'me': 96,\n",
       "          'too': 41,\n",
       "          'my': 172,\n",
       "          'opinion': 10,\n",
       "          'All': 24,\n",
       "          'You': 30,\n",
       "          'Love': 2,\n",
       "          'Will': 3,\n",
       "          'Be': 2,\n",
       "          'Carried': 1,\n",
       "          'Away': 2,\n",
       "          '5': 9,\n",
       "          'shortest': 1,\n",
       "          '(': 219,\n",
       "          'only': 74,\n",
       "          '35': 4,\n",
       "          'minutes': 9,\n",
       "          ')': 216,\n",
       "          'two': 49,\n",
       "          'ones': 9,\n",
       "          'collection': 16,\n",
       "          'Alfie': 3,\n",
       "          'Zimmer': 1,\n",
       "          'traveling': 3,\n",
       "          'salesman': 2,\n",
       "          'American': 29,\n",
       "          'Midwest': 2,\n",
       "          'He': 51,\n",
       "          'amazing': 15,\n",
       "          'hobby': 5,\n",
       "          'he': 186,\n",
       "          'tired': 3,\n",
       "          'But': 48,\n",
       "          'commits': 1,\n",
       "          'suicide': 2,\n",
       "          'everyone': 19,\n",
       "          'think': 50,\n",
       "          'makes': 32,\n",
       "          'good': 83,\n",
       "          'evocative': 2,\n",
       "          'conditions': 3,\n",
       "          'under': 7,\n",
       "          'strange': 11,\n",
       "          'captivating': 3,\n",
       "          'landscape': 3,\n",
       "          'isolated': 2,\n",
       "          'towns': 1,\n",
       "          'bleak': 1,\n",
       "          'barren': 2,\n",
       "          'admits': 2,\n",
       "          'actually': 22,\n",
       "          'something': 31,\n",
       "          'did': 51,\n",
       "          '!': 136,\n",
       "          'Death': 3,\n",
       "          'Jack': 5,\n",
       "          'Hamilton': 5,\n",
       "          '4': 6,\n",
       "          'unusual': 2,\n",
       "          'mythical': 1,\n",
       "          '1934': 1,\n",
       "          'member': 8,\n",
       "          'John': 11,\n",
       "          'Dillinger': 5,\n",
       "          'gang': 3,\n",
       "          'Homer': 2,\n",
       "          'Van': 4,\n",
       "          'Meter': 2,\n",
       "          'another': 38,\n",
       "          'purportedly': 2,\n",
       "          'all': 182,\n",
       "          'real': 24,\n",
       "          'can': 150,\n",
       "          'find': 52,\n",
       "          'lot': 40,\n",
       "          'interesting': 28,\n",
       "          'background': 9,\n",
       "          'Internet': 1,\n",
       "          'search': 2,\n",
       "          'Despite': 5,\n",
       "          'fact': 34,\n",
       "          'ruthless': 1,\n",
       "          'gangsters': 2,\n",
       "          'ourselves': 6,\n",
       "          'sympathizing': 2,\n",
       "          'approach': 15,\n",
       "          'human': 26,\n",
       "          'beings': 7,\n",
       "          'slow': 3,\n",
       "          'efforts': 2,\n",
       "          'medical': 7,\n",
       "          'help': 35,\n",
       "          'distress': 1,\n",
       "          'condition': 5,\n",
       "          'worsens': 1,\n",
       "          'powerful': 16,\n",
       "          'images': 11,\n",
       "          'Feeling': 1,\n",
       "          'Can': 6,\n",
       "          'Only': 8,\n",
       "          'Say': 1,\n",
       "          'Carol': 7,\n",
       "          'brought': 3,\n",
       "          'strict': 1,\n",
       "          'Catholic': 3,\n",
       "          'during': 17,\n",
       "          'years': 59,\n",
       "          'her': 164,\n",
       "          'marriage': 8,\n",
       "          'Bill': 2,\n",
       "          'she': 84,\n",
       "          'abortion': 1,\n",
       "          'Now': 11,\n",
       "          'supposed': 3,\n",
       "          'celebrating': 1,\n",
       "          '25th': 1,\n",
       "          'wedding': 1,\n",
       "          'anniversary': 1,\n",
       "          'subjected': 1,\n",
       "          'horrendous': 1,\n",
       "          'punishment': 4,\n",
       "          'fascinating': 18,\n",
       "          'God': 24,\n",
       "          'punishing': 1,\n",
       "          'No': 11,\n",
       "          'influence': 8,\n",
       "          'upbringing': 2,\n",
       "          'prescribed': 1,\n",
       "          'own': 54,\n",
       "          'And': 48,\n",
       "          'inflicting': 1,\n",
       "          'herself': 9,\n",
       "          'worse': 2,\n",
       "          'than': 84,\n",
       "          'benevolent': 1,\n",
       "          'assuming': 1,\n",
       "          'considers': 3,\n",
       "          'wrong': 16,\n",
       "          'summary': 1,\n",
       "          'OK': 1,\n",
       "          'professional': 7,\n",
       "          'readers': 28,\n",
       "          'total': 8,\n",
       "          'running': 13,\n",
       "          'approx': 1,\n",
       "          'hours': 8,\n",
       "          '40': 2,\n",
       "          'specified': 1,\n",
       "          'packaging': 1,\n",
       "          'included': 10,\n",
       "          'along': 17,\n",
       "          '10': 5,\n",
       "          'If': 72,\n",
       "          'you': 412,\n",
       "          'want': 50,\n",
       "          'save': 3,\n",
       "          'money': 20,\n",
       "          'dislike': 1,\n",
       "          'audio': 2,\n",
       "          'books': 83,\n",
       "          'better': 47,\n",
       "          'deal': 7,\n",
       "          'Rennie': 1,\n",
       "          'Peterse': 1,\n",
       "          'I': 867,\n",
       "          'thought': 26,\n",
       "          'previous': 7,\n",
       "          'Duncan': 1,\n",
       "          'Kincaid/Gemma': 1,\n",
       "          'James': 6,\n",
       "          'series': 34,\n",
       "          'slight': 1,\n",
       "          'disappointments': 1,\n",
       "          'Kincaid': 1,\n",
       "          'seemed': 12,\n",
       "          'relegated': 1,\n",
       "          'side': 15,\n",
       "          'character': 29,\n",
       "          'Gemma': 1,\n",
       "          'taking': 8,\n",
       "          'IN': 1,\n",
       "          'DARK': 1,\n",
       "          'HOUSE': 1,\n",
       "          'excellent': 29,\n",
       "          'mystery': 19,\n",
       "          'back': 34,\n",
       "          'equal': 3,\n",
       "          'ground': 5,\n",
       "          're-establishes': 1,\n",
       "          'partnerships': 1,\n",
       "          'fiction': 9,\n",
       "          'Once': 7,\n",
       "          'again': 38,\n",
       "          'David': 3,\n",
       "          'McCullough': 5,\n",
       "          'treatment': 3,\n",
       "          'american': 1,\n",
       "          'icon': 1,\n",
       "          'its': 62,\n",
       "          'biography': 5,\n",
       "          'reads': 6,\n",
       "          'novel': 39,\n",
       "          'McCulloughs': 1,\n",
       "          'narrative': 5,\n",
       "          'ability': 4,\n",
       "          'unmatched': 1,\n",
       "          'seems': 17,\n",
       "          'able': 19,\n",
       "          'tell': 15,\n",
       "          'Theodore': 1,\n",
       "          'Roosevelt': 2,\n",
       "          'known': 11,\n",
       "          'ages': 5,\n",
       "          'incredibly': 5,\n",
       "          'strong': 9,\n",
       "          'tough': 4,\n",
       "          'larger': 3,\n",
       "          'figure': 6,\n",
       "          'somewhat': 7,\n",
       "          'weakling': 1,\n",
       "          'covers': 19,\n",
       "          'transformation': 1,\n",
       "          'sickly': 1,\n",
       "          'child': 21,\n",
       "          'man': 30,\n",
       "          'resolve': 1,\n",
       "          'courage': 4,\n",
       "          'become': 20,\n",
       "          'President': 5,\n",
       "          'United': 3,\n",
       "          'States': 3,\n",
       "          'Best': 2,\n",
       "          'likes': 2,\n",
       "          'admires': 1,\n",
       "          'subject': 15,\n",
       "          'always': 26,\n",
       "          'case': 15,\n",
       "          'biographies': 2,\n",
       "          'family': 38,\n",
       "          'treated': 2,\n",
       "          'dignity': 1,\n",
       "          'respect': 7,\n",
       "          'deserve': 1,\n",
       "          'Highly': 7,\n",
       "          'recommended': 15,\n",
       "          'boom': 4,\n",
       "          'Mr.': 5,\n",
       "          'Dent': 1,\n",
       "          'speaks': 5,\n",
       "          'HAS': 1,\n",
       "          'begun-but': 1,\n",
       "          'starting': 6,\n",
       "          'population': 7,\n",
       "          'centers': 2,\n",
       "          'Far': 1,\n",
       "          'partially': 2,\n",
       "          'digitally': 3,\n",
       "          'controlled': 3,\n",
       "          'companies': 6,\n",
       "          'here': 37,\n",
       "          'America': 17,\n",
       "          'hit': 6,\n",
       "          'industries': 4,\n",
       "          'competitive': 2,\n",
       "          'advantage': 4,\n",
       "          'expand': 2,\n",
       "          'financial': 4,\n",
       "          'sector': 1,\n",
       "          'once': 16,\n",
       "          'enough': 31,\n",
       "          'excess': 1,\n",
       "          'labor': 4,\n",
       "          'absorbed': 1,\n",
       "          'wages': 1,\n",
       "          'go': 37,\n",
       "          'compete': 2,\n",
       "          'workers': 1,\n",
       "          'Since': 8,\n",
       "          'publicly-traded': 2,\n",
       "          'outsource': 1,\n",
       "          'significant': 3,\n",
       "          'overseas': 1,\n",
       "          'manufacturing': 1,\n",
       "          'operations': 2,\n",
       "          'using': 21,\n",
       "          'cost-savings': 1,\n",
       "          'finance': 3,\n",
       "          'implementation': 3,\n",
       "          'Sarbanes-Oxley': 2,\n",
       "          'net': 1,\n",
       "          'effect': 4,\n",
       "          'enforced': 1,\n",
       "          'global': 1,\n",
       "          'U.S.': 5,\n",
       "          'part': 23,\n",
       "          'reason': 12,\n",
       "          'why': 29,\n",
       "          'hear': 5,\n",
       "          'increasing': 2,\n",
       "          'unrest': 1,\n",
       "          'emerging': 2,\n",
       "          'markets': 4,\n",
       "          'Large': 1,\n",
       "          'portions': 2,\n",
       "          'economies': 1,\n",
       "          'reshuffled': 1,\n",
       "          'align': 1,\n",
       "          'securities': 1,\n",
       "          'laws': 4,\n",
       "          'So': 25,\n",
       "          'primarily': 3,\n",
       "          'focus': 10,\n",
       "          'around': 22,\n",
       "          ':': 79,\n",
       "          '1': 6,\n",
       "          'Baby': 2,\n",
       "          'Boomer': 1,\n",
       "          'spending': 4,\n",
       "          '2': 8,\n",
       "          'Competitive': 1,\n",
       "          'exports': 1,\n",
       "          'Reading': 7,\n",
       "          'WAR': 2,\n",
       "          'AND': 4,\n",
       "          'PEACE': 2,\n",
       "          'immense': 1,\n",
       "          'undertaking': 1,\n",
       "          'less': 12,\n",
       "          'do': 109,\n",
       "          'page': 25,\n",
       "          'numbers': 1,\n",
       "          'addressing': 4,\n",
       "          'fundamental': 9,\n",
       "          'questions': 11,\n",
       "          'entertainment': 2,\n",
       "          'Years': 1,\n",
       "          'later': 13,\n",
       "          'picked': 6,\n",
       "          'task': 4,\n",
       "          'ambitious': 2,\n",
       "          'sought': 4,\n",
       "          'know': 62,\n",
       "          'how': 121,\n",
       "          'Tolstoy': 4,\n",
       "          'justify': 2,\n",
       "          'ways': 16,\n",
       "          'During': 1,\n",
       "          'second': 19,\n",
       "          'reading': 101,\n",
       "          'examine': 5,\n",
       "          'Russian': 2,\n",
       "          'names': 11,\n",
       "          'characters': 57,\n",
       "          'events': 18,\n",
       "          'historical': 25,\n",
       "          'occurrences': 1,\n",
       "          'occupied': 1,\n",
       "          'saw': 7,\n",
       "          'hundreds': 4,\n",
       "          'individuals': 6,\n",
       "          'linked': 2,\n",
       "          'concluding': 1,\n",
       "          'Epilog': 1,\n",
       "          'scanted': 1,\n",
       "          'studied': 3,\n",
       "          'Most': 7,\n",
       "          'heavy': 1,\n",
       "          'going': 29,\n",
       "          'certainly': 17,\n",
       "          'creates': 8,\n",
       "          'key': 8,\n",
       "          'unlocking': 1,\n",
       "          'reasons': 4,\n",
       "          'things': 31,\n",
       "          'essence': 4,\n",
       "          'atom': 3,\n",
       "          'gas': 1,\n",
       "          'floating': 2,\n",
       "          'space': 12,\n",
       "          'colliding': 1,\n",
       "          'random': 3,\n",
       "          'when': 80,\n",
       "          'sufficient': 1,\n",
       "          'number': 10,\n",
       "          'act': 9,\n",
       "          'concert': 1,\n",
       "          'such': 62,\n",
       "          'Napoleon': 3,\n",
       "          'force': 6,\n",
       "          'irresistable': 1,\n",
       "          'When': 29,\n",
       "          'introduces': 5,\n",
       "          'leaders': 4,\n",
       "          'Tsar': 1,\n",
       "          'depict': 1,\n",
       "          'no': 38,\n",
       "          'free': 5,\n",
       "          'see': 44,\n",
       "          'Big': 9,\n",
       "          'Picture': 1,\n",
       "          'any': 62,\n",
       "          'clearly': 13,\n",
       "          'anyone': 41,\n",
       "          'else': 17,\n",
       "          'tragic': 6,\n",
       "          'decisions': 10,\n",
       "          'misguided': 2,\n",
       "          'collective': 2,\n",
       "          'illusion': 1,\n",
       "          'alone': 8,\n",
       "          'use': 40,\n",
       "          'divinely': 1,\n",
       "          'inspired': 5,\n",
       "          'affect': 4,\n",
       "          'lasting': 2,\n",
       "          'change': 12,\n",
       "          'acknowledge': 1,\n",
       "          'powers': 5,\n",
       "          'both': 42,\n",
       "          'limited': 11,\n",
       "          'self-deluding': 1,\n",
       "          'General': 1,\n",
       "          'Kutuzov': 1,\n",
       "          'relies': 1,\n",
       "          'looking': 26,\n",
       "          'inward': 1,\n",
       "          'toward': 5,\n",
       "          'instinct': 1,\n",
       "          'rather': 15,\n",
       "          'outward': 1,\n",
       "          'circling': 1,\n",
       "          'equally': 5,\n",
       "          'atoms': 1,\n",
       "          'drawing': 2,\n",
       "          'closer': 2,\n",
       "          'universal': 1,\n",
       "          'nature': 16,\n",
       "          'invests': 1,\n",
       "          'religious': 9,\n",
       "          'overtones': 1,\n",
       "          'synonymous': 1,\n",
       "          'bought': 12,\n",
       "          'required': 10,\n",
       "          'class': 14,\n",
       "          'am': 56,\n",
       "          '3rd': 1,\n",
       "          'Edition': 2,\n",
       "          'impressed': 5,\n",
       "          '5th': 2,\n",
       "          'appealing': 2,\n",
       "          'PMP': 1,\n",
       "          'certified': 2,\n",
       "          'reviewing': 2,\n",
       "          'exam': 3,\n",
       "          'primer': 2,\n",
       "          'head': 8,\n",
       "          'out': 98,\n",
       "          'pure': 9,\n",
       "          'PMBOK': 1,\n",
       "          'Project': 1,\n",
       "          'Management': 2,\n",
       "          'studies': 2,\n",
       "          'Directed': 1,\n",
       "          'Readings': 1,\n",
       "          'seeing': 6,\n",
       "          'PM': 2,\n",
       "          'activities': 2,\n",
       "          'suggest': 4,\n",
       "          'guidance': 3,\n",
       "          'classroom': 2,\n",
       "          'type': 14,\n",
       "          'learning': 10,\n",
       "          \"'re\": 30,\n",
       "          'experienced': 9,\n",
       "          'academic': 7,\n",
       "          'wo': 10,\n",
       "          'feed': 3,\n",
       "          'your': 96,\n",
       "          'intellect': 1,\n",
       "          'Buy': 4,\n",
       "          'With': 14,\n",
       "          'sensationalism': 1,\n",
       "          'relentless': 1,\n",
       "          'obfuscation': 1,\n",
       "          'realities': 2,\n",
       "          'media': 7,\n",
       "          'found': 52,\n",
       "          'exhillarating': 1,\n",
       "          'provoking': 1,\n",
       "          '...': 54,\n",
       "          \"'m\": 22,\n",
       "          'sure': 9,\n",
       "          \"'ll\": 32,\n",
       "          'revisit': 1,\n",
       "          'prominently': 1,\n",
       "          'displayed': 1,\n",
       "          'bookshelf': 1,\n",
       "          'Worth': 2,\n",
       "          'every': 38,\n",
       "          'penny..': 1,\n",
       "          'Tony': 3,\n",
       "          'Sweet': 2,\n",
       "          'photographs': 5,\n",
       "          'breathtaking': 1,\n",
       "          'As': 51,\n",
       "          'photo': 1,\n",
       "          'associated': 6,\n",
       "          'rationale': 2,\n",
       "          'chose': 3,\n",
       "          'particular': 8,\n",
       "          'exposure': 1,\n",
       "          'composed': 2,\n",
       "          'certain': 10,\n",
       "          'realize': 8,\n",
       "          'keen': 2,\n",
       "          'awareness': 1,\n",
       "          'environment': 6,\n",
       "          'obvious': 8,\n",
       "          'care': 11,\n",
       "          'places': 10,\n",
       "          'work': 74,\n",
       "          'glad': 12,\n",
       "          'shares': 4,\n",
       "          'technical': 8,\n",
       "          'photographers': 2,\n",
       "          'levels': 4,\n",
       "          'gain': 7,\n",
       "          'studying': 4,\n",
       "          'runner': 1,\n",
       "          'may': 48,\n",
       "          'mental': 7,\n",
       "          'benefits': 8,\n",
       "          'provides': 21,\n",
       "          'felt': 17,\n",
       "          'spiritual': 8,\n",
       "          'presence': 1,\n",
       "          'while': 41,\n",
       "          'perhaps': 10,\n",
       "          'saying': 6,\n",
       "          'prayer': 2,\n",
       "          'working': 11,\n",
       "          'personal': 25,\n",
       "          'problems': 11,\n",
       "          'trail': 2,\n",
       "          'extraordinary': 4,\n",
       "          'Joslin': 1,\n",
       "          'explores': 8,\n",
       "          'mind/body': 1,\n",
       "          'connection': 5,\n",
       "          'sport': 1,\n",
       "          'meditative': 1,\n",
       "          'techniques': 7,\n",
       "          'developed': 6,\n",
       "          'rich': 8,\n",
       "          'experiences': 8,\n",
       "          'excerpts': 1,\n",
       "          'journal': 2,\n",
       "          'well': 75,\n",
       "          'thought-out': 1,\n",
       "          'written': 58,\n",
       "          'humility': 2,\n",
       "          'changed': 7,\n",
       "          'day': 23,\n",
       "          'Jeffrey': 1,\n",
       "          'worth': 14,\n",
       "          'version': 4,\n",
       "          'edition': 16,\n",
       "          'lots': 5,\n",
       "          'inside': 5,\n",
       "          'CLR': 1,\n",
       "          'must-read': 2,\n",
       "          'job': 26,\n",
       "          'interview': 1,\n",
       "          'interviewers': 1,\n",
       "          'borrowed': 1,\n",
       "          'CD': 5,\n",
       "          'library': 21,\n",
       "          'half': 13,\n",
       "          'ordered': 3,\n",
       "          'Excellent': 2,\n",
       "          'highly': 29,\n",
       "          'Helps': 1,\n",
       "          'put': 30,\n",
       "          'realistic': 6,\n",
       "          'perspective': 11,\n",
       "          'millionaires': 1,\n",
       "          'childhood': 7,\n",
       "          'teacher': 6,\n",
       "          'Lynch': 4,\n",
       "          'wrote': 11,\n",
       "          'classic': 11,\n",
       "          'Spanish-American': 1,\n",
       "          'Revolutions': 1,\n",
       "          '1808-1826': 1,\n",
       "          'masterfully': 1,\n",
       "          'describes': 9,\n",
       "          'independence': 3,\n",
       "          'Latin': 2,\n",
       "          'Spain': 1,\n",
       "          'starts': 4,\n",
       "          'Rio': 1,\n",
       "          'de': 2,\n",
       "          'La': 1,\n",
       "          'Plata': 1,\n",
       "          'ends': 4,\n",
       "          'Mexico': 2,\n",
       "          'Central': 2,\n",
       "          'Curiously': 1,\n",
       "          'note': 5,\n",
       "          'common': 17,\n",
       "          'pattern': 2,\n",
       "          'stratified': 1,\n",
       "          'societies': 4,\n",
       "          'Spanish': 3,\n",
       "          'merchants': 2,\n",
       "          'complete': 10,\n",
       "          'harmony': 1,\n",
       "          'Creole': 1,\n",
       "          'ruling': 1,\n",
       "          'reluctance': 1,\n",
       "          'Monarchy': 1,\n",
       "          'liberals': 1,\n",
       "          'basically': 2,\n",
       "          'motivated': 4,\n",
       "          'social': 6,\n",
       "          'interests': 2,\n",
       "          ...})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist # see http://www.nltk.org/api/nltk.html#module-nltk.probability\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "from functools import reduce # see https://docs.python.org/3/library/functools.html\n",
    "\n",
    "#Helper function. Given a list of reviews, return a list of all the words in those reviews\n",
    "#To understand this look at the description of functools.reduce in https://docs.python.org/3/library/functools.html\n",
    "def get_all_words(amazon_reviews):\n",
    "    #print(reduce(lambda words,review: words + review.words(), amazon_reviews, []))\n",
    "    return reduce(lambda words,review: words + review.words(), amazon_reviews, [])\n",
    "\n",
    "#A frequency distribution over all words in positive book reviews\n",
    "pos_freqdist = FreqDist(get_all_words(pos_train))\n",
    "neg_freqdist = FreqDist(get_all_words(neg_train))\n",
    "pos_freqdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the blank code cell below write code that uses the frequency lists, `pos_freqdist` and `neg_freqdist`, created in the above cell and `my_positive_word_list` and `my_negative_word_list` that you manually created earlier to determine whether or not the review data conforms to your expectations. In particular, whether:\n",
    "- the words you expected to indicate positive sentiment actually occur more frequently in positive reviews than negative reviews\n",
    "- the words you expected to indicate negative sentiment actually occur more frequently in negative reviews than positive reviews.\n",
    "\n",
    "Display your findings in a table using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_exp(freq_dist1, freq_dist2, word_list):\n",
    "    match = [freq_dist1[word] for word in word_list]\n",
    "    mismatch = [freq_dist2[word] for word in word_list]\n",
    "    expected = [match[i] > mismatch[i] for i in range(len(wordlist))]\n",
    "    df = pd.DataFrame(list(zip_longest(word_list, match, mismatch, exected)), columns = [\"word\", \"freq pos\", \"freq neg\", \"exp\"])\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/check_expectations_partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos Word</th>\n",
       "      <th>Freq in Pos</th>\n",
       "      <th>Freq in Neg</th>\n",
       "      <th>Expected?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good</td>\n",
       "      <td>83</td>\n",
       "      <td>116</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great</td>\n",
       "      <td>106</td>\n",
       "      <td>51</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lovely</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>marvellous</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>perfect</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spectacular</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>best</td>\n",
       "      <td>65</td>\n",
       "      <td>31</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cool</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Pos Word  Freq in Pos  Freq in Neg  Expected?\n",
       "0         good           83          116      False\n",
       "1        great          106           51       True\n",
       "2       lovely            1            2      False\n",
       "3    wonderful           21            5       True\n",
       "4   marvellous            0            0      False\n",
       "5      perfect            7            8      False\n",
       "6  spectacular            3            0       True\n",
       "7         best           65           31       True\n",
       "8    beautiful           13            9       True\n",
       "9         cool            3            1       True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neg Word</th>\n",
       "      <th>Freq in Neg</th>\n",
       "      <th>Freq in Pos</th>\n",
       "      <th>Expected?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bad</td>\n",
       "      <td>46</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>terrible</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>awful</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disgusting</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worst</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>unpleasant</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>poor</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>boring</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Neg Word  Freq in Neg  Freq in Pos  Expected?\n",
       "0         bad           46           11       True\n",
       "1    terrible            8            6       True\n",
       "2       awful            8            2       True\n",
       "3  disgusting            1            0       True\n",
       "4       worst           14            2       True\n",
       "5  unpleasant            2            0       True\n",
       "6        poor           12            8       True\n",
       "7      boring           27            3       True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load solutions/check_expectations\n",
    "def check_expectations(word_list,freqdist1,freqdist2,headers):\n",
    "    match_freq = [freqdist1[word] for word in word_list]\n",
    "    mismatch_freq = [freqdist2[word] for word in word_list]\n",
    "    as_expected = [match_freq[i]>mismatch_freq[i] for i in range(len(word_list))]\n",
    "    headers.append('Expected?')\n",
    "    df = pd.DataFrame(list(zip_longest(word_list,match_freq,mismatch_freq,as_expected)), columns=headers)\n",
    "    display(df,\"\\n\")\n",
    "\n",
    "pos_freqs_of_my_negative = [pos_freqdist[word] for word in my_negative_word_list]\n",
    "neg_freqs_of_my_negative = [neg_freqdist[word] for word in my_negative_word_list]\n",
    "headers = [\"Pos Word\",\"Freq in Pos\", \"Freq in Neg\"]\n",
    "check_expectations(my_positive_word_list,pos_freqdist,neg_freqdist,headers)\n",
    "headers = [\"Neg Word\",\"Freq in Neg\", \"Freq in Pos\"]\n",
    "check_expectations(my_negative_word_list,neg_freqdist,pos_freqdist,headers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "The cell below is a copy of the cell that appeared just before the last exercise.\n",
    "\n",
    "Add two functions to this code.\n",
    "\n",
    "- `most_frequent_words` - this function should take two arguments: a frequency distribution and a natural number, k. It should return the top k most frequent  words in the frequency distribution. You can use the `most_common` method for this, but you will need to aware of what this method returns.\n",
    "- `words_above_threshold` - this function also takes two arguments: a frequency distribution and a natural number, k. It should return all of the words that have a frequency greater than k.\n",
    "\n",
    "Remove punctuation and stopwords from consideration. You can re-use code from near the end of Topic 1 notebook.\n",
    "Using the training data, create two sets of positive and negative word lists using these functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TopPos</th>\n",
       "      <th>TopNeg</th>\n",
       "      <th>AbovePos</th>\n",
       "      <th>AboveNeg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>It</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book</td>\n",
       "      <td>book</td>\n",
       "      <td>like</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The</td>\n",
       "      <td>The</td>\n",
       "      <td>This</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>read</td>\n",
       "      <td>read</td>\n",
       "      <td>book</td>\n",
       "      <td>read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This</td>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>This</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>one</td>\n",
       "      <td>like</td>\n",
       "      <td>The</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>It</td>\n",
       "      <td>would</td>\n",
       "      <td>read</td>\n",
       "      <td>much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>many</td>\n",
       "      <td>It</td>\n",
       "      <td>many</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>great</td>\n",
       "      <td>This</td>\n",
       "      <td>great</td>\n",
       "      <td>could</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>story</td>\n",
       "      <td>books</td>\n",
       "      <td>story</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>like</td>\n",
       "      <td>much</td>\n",
       "      <td>I</td>\n",
       "      <td>It</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>reading</td>\n",
       "      <td>good</td>\n",
       "      <td>reading</td>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>time</td>\n",
       "      <td>story</td>\n",
       "      <td>None</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>also</td>\n",
       "      <td>time</td>\n",
       "      <td>None</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>life</td>\n",
       "      <td>could</td>\n",
       "      <td>None</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>would</td>\n",
       "      <td>even</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>good</td>\n",
       "      <td>people</td>\n",
       "      <td>None</td>\n",
       "      <td>even</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>books</td>\n",
       "      <td>author</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>first</td>\n",
       "      <td>really</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>much</td>\n",
       "      <td>reading</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>really</td>\n",
       "      <td>If</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>people</td>\n",
       "      <td>get</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>quot</td>\n",
       "      <td>many</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>way</td>\n",
       "      <td>well</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>well</td>\n",
       "      <td>better</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>work</td>\n",
       "      <td>But</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>even</td>\n",
       "      <td>characters</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>If</td>\n",
       "      <td>make</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>In</td>\n",
       "      <td>first</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>best</td>\n",
       "      <td>He</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>writing</td>\n",
       "      <td>money</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>go</td>\n",
       "      <td>take</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>ever</td>\n",
       "      <td>someone</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>need</td>\n",
       "      <td>things</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>help</td>\n",
       "      <td>something</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>never</td>\n",
       "      <td>might</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>For</td>\n",
       "      <td>seems</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>fact</td>\n",
       "      <td>nothing</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>series</td>\n",
       "      <td>made</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>back</td>\n",
       "      <td>another</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>children</td>\n",
       "      <td>see</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>important</td>\n",
       "      <td>anything</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>state</td>\n",
       "      <td>point</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>easy</td>\n",
       "      <td>actually</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>far</td>\n",
       "      <td>use</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>though</td>\n",
       "      <td>lot</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>come</td>\n",
       "      <td>last</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>makes</td>\n",
       "      <td>without</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>end</td>\n",
       "      <td>tell</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>experience</td>\n",
       "      <td>thing</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>something</td>\n",
       "      <td>What</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>enough</td>\n",
       "      <td>long</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>things</td>\n",
       "      <td>While</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>say</td>\n",
       "      <td>far</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>give</td>\n",
       "      <td>You</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>You</td>\n",
       "      <td>believe</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>man</td>\n",
       "      <td>fact</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>put</td>\n",
       "      <td>However</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>must</td>\n",
       "      <td>feel</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>feel</td>\n",
       "      <td>go</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TopPos      TopNeg AbovePos AboveNeg\n",
       "0            I           I       It     book\n",
       "1         book        book     like        I\n",
       "2          The         The     This    would\n",
       "3         read        read     book     read\n",
       "4         This         one      one     This\n",
       "5          one        like      The      one\n",
       "6           It       would     read     much\n",
       "7         many          It     many      The\n",
       "8        great        This    great    could\n",
       "9        story       books    story     like\n",
       "10        like        much        I       It\n",
       "11     reading        good  reading   people\n",
       "12        time       story     None     time\n",
       "13        also        time     None    books\n",
       "14        life       could     None     good\n",
       "15       would        even     None    story\n",
       "16        good      people     None     even\n",
       "17       books      author     None     None\n",
       "18       first      really     None     None\n",
       "19        much     reading     None     None\n",
       "20      really          If     None     None\n",
       "21      people         get     None     None\n",
       "22        quot        many     None     None\n",
       "23         way        well     None     None\n",
       "24        well      better     None     None\n",
       "25        work         But     None     None\n",
       "26        even  characters     None     None\n",
       "27          If        make     None     None\n",
       "28          In       first     None     None\n",
       "29        best          He     None     None\n",
       "..         ...         ...      ...      ...\n",
       "70     writing       money     None     None\n",
       "71          go        take     None     None\n",
       "72        ever     someone     None     None\n",
       "73        need      things     None     None\n",
       "74        help   something     None     None\n",
       "75       never       might     None     None\n",
       "76         For       seems     None     None\n",
       "77        fact     nothing     None     None\n",
       "78      series        made     None     None\n",
       "79        back     another     None     None\n",
       "80    children         see     None     None\n",
       "81   important    anything     None     None\n",
       "82       state       point     None     None\n",
       "83        easy    actually     None     None\n",
       "84         far         use     None     None\n",
       "85      though         lot     None     None\n",
       "86        come        last     None     None\n",
       "87       makes     without     None     None\n",
       "88         end        tell     None     None\n",
       "89  experience       thing     None     None\n",
       "90   something        What     None     None\n",
       "91      enough        long     None     None\n",
       "92      things       While     None     None\n",
       "93         say         far     None     None\n",
       "94        give         You     None     None\n",
       "95         You     believe     None     None\n",
       "96         man        fact     None     None\n",
       "97         put     However     None     None\n",
       "98        must        feel     None     None\n",
       "99        feel          go     None     None\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## from nltk.probability import FreqDist # see http://www.nltk.org/api/nltk.html#module-nltk.probability\n",
    "from nltk.probability import FreqDist # see http://www.nltk.org/api/nltk.html#module-nltk.probability\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "from functools import reduce # see https://docs.python.org/3/library/functools.html\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "def remove_stopwords_and_punctuation(words):\n",
    "    return [w for w in words if w.isalpha() and w not in stopwords]\n",
    "\n",
    "#Helper function. Given a list of reviews, return a list of all the words in those reviews\n",
    "#To understand this look at the description of functools.reduce in https://docs.python.org/3/library/functools.html\n",
    "def get_all_words(amazon_reviews):\n",
    "    return reduce(lambda words,review: words + review.words(), amazon_reviews, [])\n",
    "#A frequency distribution over all words in positive book reviews\n",
    "pos_freqdist = FreqDist(remove_stopwords_and_punctuation(get_all_words(pos_train)))\n",
    "neg_freqdist = FreqDist(remove_stopwords_and_punctuation(get_all_words(neg_train)))\n",
    "\n",
    "def most_frequent_words(freq_dist, k):\n",
    "    return [word for word, count in freq_dist.most_common(k)]               \n",
    "\n",
    "def words_above_threshold(freq_dist, k):\n",
    "    return [word for word in freq_dist if freq_dist[word] > k]\n",
    "\n",
    "top_pos = most_frequent_words(pos_freqdist,100)\n",
    "top_neg = most_frequent_words(neg_freqdist,100)\n",
    "above_pos = words_above_threshold(pos_freqdist,100)\n",
    "above_neg = words_above_threshold(neg_freqdist,100)\n",
    "display(pd.DataFrame(list(zip_longest(top_pos,top_neg,above_pos,above_neg)),columns=[\"TopPos\",\"TopNeg\",\"AbovePos\",\"AboveNeg\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TopPos</th>\n",
       "      <th>TopNeg</th>\n",
       "      <th>AbovePos</th>\n",
       "      <th>AboveNeg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>It</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book</td>\n",
       "      <td>book</td>\n",
       "      <td>like</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The</td>\n",
       "      <td>The</td>\n",
       "      <td>This</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>read</td>\n",
       "      <td>read</td>\n",
       "      <td>book</td>\n",
       "      <td>read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This</td>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>This</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>one</td>\n",
       "      <td>like</td>\n",
       "      <td>The</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>It</td>\n",
       "      <td>would</td>\n",
       "      <td>read</td>\n",
       "      <td>much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>many</td>\n",
       "      <td>It</td>\n",
       "      <td>many</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>great</td>\n",
       "      <td>This</td>\n",
       "      <td>great</td>\n",
       "      <td>could</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>story</td>\n",
       "      <td>books</td>\n",
       "      <td>story</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>like</td>\n",
       "      <td>much</td>\n",
       "      <td>I</td>\n",
       "      <td>It</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>reading</td>\n",
       "      <td>good</td>\n",
       "      <td>reading</td>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>time</td>\n",
       "      <td>story</td>\n",
       "      <td>None</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>also</td>\n",
       "      <td>time</td>\n",
       "      <td>None</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>life</td>\n",
       "      <td>could</td>\n",
       "      <td>None</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>would</td>\n",
       "      <td>even</td>\n",
       "      <td>None</td>\n",
       "      <td>story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>good</td>\n",
       "      <td>people</td>\n",
       "      <td>None</td>\n",
       "      <td>even</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>books</td>\n",
       "      <td>author</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>first</td>\n",
       "      <td>really</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>much</td>\n",
       "      <td>reading</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>really</td>\n",
       "      <td>If</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>people</td>\n",
       "      <td>get</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>quot</td>\n",
       "      <td>many</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>way</td>\n",
       "      <td>well</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>well</td>\n",
       "      <td>better</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>work</td>\n",
       "      <td>But</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>even</td>\n",
       "      <td>characters</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>If</td>\n",
       "      <td>make</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>In</td>\n",
       "      <td>first</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>best</td>\n",
       "      <td>He</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>writing</td>\n",
       "      <td>money</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>go</td>\n",
       "      <td>take</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>ever</td>\n",
       "      <td>someone</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>need</td>\n",
       "      <td>things</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>help</td>\n",
       "      <td>something</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>never</td>\n",
       "      <td>might</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>For</td>\n",
       "      <td>seems</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>fact</td>\n",
       "      <td>nothing</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>series</td>\n",
       "      <td>made</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>back</td>\n",
       "      <td>another</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>children</td>\n",
       "      <td>see</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>important</td>\n",
       "      <td>anything</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>state</td>\n",
       "      <td>point</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>easy</td>\n",
       "      <td>actually</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>far</td>\n",
       "      <td>use</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>though</td>\n",
       "      <td>lot</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>come</td>\n",
       "      <td>last</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>makes</td>\n",
       "      <td>without</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>end</td>\n",
       "      <td>tell</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>experience</td>\n",
       "      <td>thing</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>something</td>\n",
       "      <td>What</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>enough</td>\n",
       "      <td>long</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>things</td>\n",
       "      <td>While</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>say</td>\n",
       "      <td>far</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>give</td>\n",
       "      <td>You</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>You</td>\n",
       "      <td>believe</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>man</td>\n",
       "      <td>fact</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>put</td>\n",
       "      <td>However</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>must</td>\n",
       "      <td>feel</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>feel</td>\n",
       "      <td>go</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TopPos      TopNeg AbovePos AboveNeg\n",
       "0            I           I       It     book\n",
       "1         book        book     like        I\n",
       "2          The         The     This    would\n",
       "3         read        read     book     read\n",
       "4         This         one      one     This\n",
       "5          one        like      The      one\n",
       "6           It       would     read     much\n",
       "7         many          It     many      The\n",
       "8        great        This    great    could\n",
       "9        story       books    story     like\n",
       "10        like        much        I       It\n",
       "11     reading        good  reading   people\n",
       "12        time       story     None     time\n",
       "13        also        time     None    books\n",
       "14        life       could     None     good\n",
       "15       would        even     None    story\n",
       "16        good      people     None     even\n",
       "17       books      author     None     None\n",
       "18       first      really     None     None\n",
       "19        much     reading     None     None\n",
       "20      really          If     None     None\n",
       "21      people         get     None     None\n",
       "22        quot        many     None     None\n",
       "23         way        well     None     None\n",
       "24        well      better     None     None\n",
       "25        work         But     None     None\n",
       "26        even  characters     None     None\n",
       "27          If        make     None     None\n",
       "28          In       first     None     None\n",
       "29        best          He     None     None\n",
       "..         ...         ...      ...      ...\n",
       "70     writing       money     None     None\n",
       "71          go        take     None     None\n",
       "72        ever     someone     None     None\n",
       "73        need      things     None     None\n",
       "74        help   something     None     None\n",
       "75       never       might     None     None\n",
       "76         For       seems     None     None\n",
       "77        fact     nothing     None     None\n",
       "78      series        made     None     None\n",
       "79        back     another     None     None\n",
       "80    children         see     None     None\n",
       "81   important    anything     None     None\n",
       "82       state       point     None     None\n",
       "83        easy    actually     None     None\n",
       "84         far         use     None     None\n",
       "85      though         lot     None     None\n",
       "86        come        last     None     None\n",
       "87       makes     without     None     None\n",
       "88         end        tell     None     None\n",
       "89  experience       thing     None     None\n",
       "90   something        What     None     None\n",
       "91      enough        long     None     None\n",
       "92      things       While     None     None\n",
       "93         say         far     None     None\n",
       "94        give         You     None     None\n",
       "95         You     believe     None     None\n",
       "96         man        fact     None     None\n",
       "97         put     However     None     None\n",
       "98        must        feel     None     None\n",
       "99        feel          go     None     None\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load solutions/make_word_lists\n",
    "from nltk.probability import FreqDist # see http://www.nltk.org/api/nltk.html#module-nltk.probability\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "from functools import reduce # see https://docs.python.org/3/library/functools.html\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords_and_punctuation(words):\n",
    "    return [w for w in words if w.isalpha() and w not in stopwords]\n",
    "\n",
    "#Helper function. Given a list of reviews, return a list of all the words in those reviews\n",
    "#To understand this look at the description of functools.reduce in https://docs.python.org/3/library/functools.html\n",
    "def get_all_words(amazon_reviews):\n",
    "    return reduce(lambda words,review: words + review.words(), amazon_reviews, [])\n",
    "\n",
    "#A frequency distribution over all words in positive book reviews\n",
    "pos_freqdist = FreqDist(remove_stopwords_and_punctuation(get_all_words(pos_train)))\n",
    "neg_freqdist = FreqDist(remove_stopwords_and_punctuation(get_all_words(neg_train)))\n",
    "\n",
    "def most_frequent_words(freqdist,k):\n",
    "    return [word for word,count in freqdist.most_common(k)]\n",
    "\n",
    "def words_above_threshold(freqdist,k):\n",
    "    return [word for word in freqdist if freqdist[word]>k]\n",
    "\n",
    "\n",
    "top_pos = most_frequent_words(pos_freqdist,100)\n",
    "top_neg = most_frequent_words(neg_freqdist,100)\n",
    "above_pos = words_above_threshold(pos_freqdist,100)\n",
    "above_neg = words_above_threshold(neg_freqdist,100)\n",
    "display(pd.DataFrame(list(zip_longest(top_pos,top_neg,above_pos,above_neg)),columns=[\"TopPos\",\"TopNeg\",\"AbovePos\",\"AboveNeg\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a word list based classifier\n",
    "Now you have a number of word lists for use with a classifier. The following code can be used as the basis for creating a word list based classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify.api import ClassifierI\n",
    "import random\n",
    "\n",
    "class SimpleClassifier(ClassifierI): \n",
    "\n",
    "    def __init__(self, pos, neg): \n",
    "        self._pos = pos \n",
    "        self._neg = neg \n",
    "\n",
    "    def classify(self, words): \n",
    "        score = 0\n",
    "        \n",
    "        # add code here that assigns an appropriate value to score\n",
    "        \n",
    "        return \"N\" if score < 0 else \"P\" \n",
    "\n",
    "    def batch_classify(self, docs): \n",
    "        return [self.classify(doc.words() if hasattr(doc, 'words') else doc) for doc in docs] \n",
    "\n",
    "    def labels(self): \n",
    "        return (\"P\", \"N\")\n",
    "\n",
    "#Example usage:\n",
    "\n",
    "classifier = SimpleClassifier(top_pos, top_neg)\n",
    "classifier.classify(\"I read the book\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Copy the above code cell and move it to below this one. Then complete the `classify` method in the above code as specified below.\n",
    "- Test your classifier on several very simple hand-crafted examples to verify that you have implemented `classify` correctly.\n",
    "\n",
    "The classifier is initialised with a list of positive words, and a list of negative words. The words of a document are passed to the `classify` method (which is partially completed in the above code fragment). The `classify` method should be defined so that each occurrence of a negative word decrements `score`, and each occurrence of a positive word increments `score`. \n",
    "- For `score` less than 0, an \"`N`\" for negative should be returned.\n",
    "- For `score` greater than 0,  \"`P`\" for positive should returned.\n",
    "- For `score` of 0, the classification decision should be made randomly (see https://docs.python.org/3/library/random.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify.api import ClassifierI\n",
    "import random\n",
    "\n",
    "class SimpleClassifier(ClassifierI): \n",
    "\n",
    "    def __init__(self, pos, neg): \n",
    "        self._pos = pos \n",
    "        self._neg = neg \n",
    "\n",
    "    def classify(self, words): \n",
    "        score = 0\n",
    "        for word in words:\n",
    "            if word in self._pos:\n",
    "                score += 1\n",
    "            elif word in self._neg:\n",
    "                score -= 1        \n",
    "        if score > 0:\n",
    "            return \"P\"\n",
    "        elif score < 0:\n",
    "            return \"N\"\n",
    "        else :\n",
    "            return random.choice([\"N\", \"P\"])\n",
    "        \n",
    "    def batch_classify(self, docs): \n",
    "        return [self.classify(doc.words() if hasattr(doc, 'words') else doc) for doc in docs] \n",
    "\n",
    "    def labels(self): \n",
    "        return (\"P\", \"N\")\n",
    "\n",
    "#Example usage:\n",
    "\n",
    "classifier = SimpleClassifier(top_pos, top_neg)\n",
    "classifier.classify(\"I read the book\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load solutions/simple_classify\n",
    "from nltk.classify.api import ClassifierI\n",
    "import random\n",
    "\n",
    "class SimpleClassifier(ClassifierI): \n",
    "\n",
    "    def __init__(self, pos, neg): \n",
    "        self._pos = pos \n",
    "        self._neg = neg \n",
    "\n",
    "    def classify(self, words): \n",
    "        score = 0\n",
    "        \n",
    "        # add code here that assigns an appropriate value to score\n",
    "        for word in words:\n",
    "            if word in self._pos:\n",
    "                score += 1\n",
    "            if word in self._neg:\n",
    "                score -= 1\n",
    "        if score < 0:\n",
    "            return \"N\"\n",
    "        if score > 0:\n",
    "            return \"P\"\n",
    "        return random.choice([\"N\",\"P\"])\n",
    "\n",
    "    def batch_classify(self, docs): \n",
    "        return [self.classify(doc.words() if hasattr(doc, 'words') else doc) for doc in docs] \n",
    "\n",
    "    def labels(self): \n",
    "        return (\"P\", \"N\")\n",
    "\n",
    "#Example usage:\n",
    "\n",
    "classifier = SimpleClassifier(top_pos, top_neg)\n",
    "classifier.classify(\"I read the book\".split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating word list based classifier\n",
    "Below is code that uses an evaluation function in order to determine how well your classifier performs. The function returns the <b>accuracy</b> of a classifier. The accuracy metric is defined as the proportion of documents that were correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6143939393939394\n"
     ]
    }
   ],
   "source": [
    "from sussex_nltk.stats import evaluate_wordlist_classifier\n",
    "\n",
    "#Create a new classifier with your words lists\n",
    "book_classifier = SimpleClassifier(top_pos, top_neg)\n",
    "\n",
    "#Evaluate classifier\n",
    "#The function requires three arguments:\n",
    "# 1. Word list based classifer\n",
    "# 2. A list (or generator) of positive AmazonReview objects\n",
    "# 3. A list (or generator) of negative AmazonReview objects\n",
    "score = evaluate_wordlist_classifier(book_classifier, pos_test, neg_test)  \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise  \n",
    "\n",
    "Use the blank cell below to perform the following two experiments:\n",
    "- Evaluate the performance of a classifier using hand-crafted lists.\n",
    "- Evaluate the performance of a classifier using lists derived from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Hand-Crafted lists classifer is 0.5765\n",
      "The accuracy of the Top-k lists classifer is 0.6212\n",
      "The accuracy of the Above-k lists classifer is 0.5273\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/classifier_test\n",
    "from sussex_nltk.stats import evaluate_wordlist_classifier\n",
    "\n",
    "experiments = [(\"Hand-Crafted lists\",my_positive_word_list,my_negative_word_list),\n",
    "               (\"Top-k lists\",top_pos,top_neg),\n",
    "               (\"Above-k lists\",above_pos,above_neg)]\n",
    "\n",
    "\n",
    "for description,pos_list,neg_list in experiments:\n",
    "    #Create a new classifier with your words lists\n",
    "    classifier = SimpleClassifier(pos_list, neg_list)\n",
    "    score = evaluate_wordlist_classifier(classifier, pos_test, neg_test)\n",
    "    print(\"The accuracy of the {0} classifer is {1:.4f}\".format(description,score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Na√Øve Bayes classifiers\n",
    "We now turn to a different model of document classification known as Na√Øve Bayes.\n",
    "\n",
    "Before we apply them to Amazon reviews, we need to implement them!\n",
    "\n",
    "We will introduce them through a very simple example dataset involving documents that are either about the weather or football. The classifier will be trained to distinguish these two topics from one another.\n",
    "\n",
    "There are, therefore, two classes `weather` and `football`. The classifier's job is to determine whether a document that it is given is in the class `weather` or in the class `football`.\n",
    "\n",
    "We give the classifier examples of documents in the `weather` class, and examples of documents in the `football` class. For now, to keep things simple, our so-called documents will be very short phrases.\n",
    "\n",
    "Run the following cell to set up some training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some test data to get us started. It is deliberately VERY simple. \n",
    "\n",
    "weather_sents_train = [\n",
    "    \"today it is raining\",\n",
    "    \"looking cloudy today\",\n",
    "    \"it is nice weather\",\n",
    "]\n",
    "\n",
    "football_sents_train = [\n",
    "    \"city looking good\",\n",
    "    \"advantage united\",\n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "One of the simplying assumptions that is made with Na√Øve Bayes classification is that each document is taken to be a so-called bag of words. What this means is that the word order is ignored, and we are not taking into account the number of times a word appears in a document. Note that there are variants of Na√Øve Bayes where the number of times a words appears in a document is taken into account, but we will not consider that case here.\n",
    "\n",
    "Given the bag-of-words assumption, we will represent each training document as a pair consisting of a dictionary that maps each word that appears in the document to `True`, and a string denoting the class of the document. \n",
    "\n",
    "### Exercise\n",
    "In the cell below, write code that achieves this. Create three dictionaries:\n",
    "- `weather_data_train`: a dictionary containing the data for documents in the class `weather`;\n",
    "- `football_data_train`: a dictionary containing the data for documents in the class `football`;\n",
    "- `train_data`: which is simply `weather_data_train + football_data_train`\n",
    "\n",
    "Hint: this can be done with nested list comprehensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'is': True, 'it': True, 'raining': True, 'today': True}, 'weather'),\n",
       " ({'cloudy': True, 'looking': True, 'today': True}, 'weather'),\n",
       " ({'is': True, 'it': True, 'nice': True, 'weather': True}, 'weather'),\n",
       " ({'city': True, 'good': True, 'looking': True}, 'football'),\n",
       " ({'advantage': True, 'united': True}, 'football')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_data_train = [(dict([(word, True) for word in sent.split()]), \"weather\") for sent in weather_sents_train] \n",
    "football_data_train = [(dict([(word, True) for word in sent.split()]), \"football\") for sent in football_sents_train] \n",
    "train_data = weather_data_train + football_data_train\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/format_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How a Na√Øve Bayes classifier works\n",
    "\n",
    "We will now look at how a NB work, using our `weather` versus `football` classification task.\n",
    "\n",
    "In order to classify a document, $d$, we need to determine which of these probabilities is greatest:\n",
    "\n",
    "$$P(\\,\\mbox{weather}\\,|\\,d) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad P(\\,\\mbox{football}\\,|\\,d)$$\n",
    "\n",
    "$d$ could, for example, be the string \"today is looking cloudy\", which would give us:\n",
    "\n",
    "$$P(\\,\\mbox{weather}\\,|\\,\\mbox{\"today is looking cloudy\"}) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad P(\\,\\mbox{football}\\,|\\,\\mbox{\"today is looking cloudy\"})$$\n",
    "\n",
    "The idea is that if the term on the left is higher then the document is in category `weather`, and if the term on the right is higher then the document is in category `football`.\n",
    "\n",
    "$P(X|Y)$ means the probability of $X$ given $Y$. So, $P(\\,\\mbox{weather}\\,|\\,d)$ means the probability, given a document $d$, of $d$ being of class `weather`.\n",
    "\n",
    "We are going to use something called Bayes' rule which states that:\n",
    "\n",
    "$$P(X|Y) = \\frac{P(Y|X)\\cdot P(X)}{P(Y)}$$\n",
    "\n",
    "Applying Bayes' rule to our problem leads to the following comparision:\n",
    "\n",
    "$$\\frac{P(\\,d\\,|\\,\\mbox{weather}\\,)\\cdot P(\\,\\mbox{weather}\\,)}{p(d)} \\qquad\\qquad \\mbox{versus} \\qquad\\qquad \\frac{P(\\,d\\,|\\,\\mbox{football}\\,)\\cdot P(\\,\\mbox{football}\\,)}{p(d)}$$\n",
    "\n",
    "Since both sides are being divided by the same thing, we only need to make the following comparision:\n",
    "\n",
    "$$P(\\,d\\,|\\,\\mbox{weather}\\,)\\cdot P(\\,\\mbox{weather}\\,) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad P(\\,d\\,|\\,\\mbox{football}\\,)\\cdot P(\\,\\mbox{football}\\,)$$\n",
    "\n",
    "Let's just look at what each of these probabilities mean?\n",
    "\n",
    "1. $P(\\,d\\,|\\,\\mbox{weather}\\,)$: this is the probability of a document in the `weather` category being the document $d$\n",
    "\n",
    "2. $P(\\,d\\,|\\,\\mbox{football}\\,)$: this is the probability of a document in the `football` category being the document $d$\n",
    "\n",
    "3. $P(\\,\\mbox{weather}\\,)$: this is the probability of a randomly selected document being of category `weather`.\n",
    "\n",
    "4. $P(\\,\\mbox{football}\\,)$: this is the probability of a randomly selected document being of category `football`.\n",
    "\n",
    "How are we going to obtain these probabilities? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class priors\n",
    "\n",
    "We have established that we need to know $P(\\,\\mbox{weather}\\,)$ and $P(\\,\\mbox{football}\\,)$. These are called the class priors. Let's see how we can obtain (estimated) values for these probabilities from our training data.\n",
    "\n",
    "The classifier has seen three documents of class `weather` and two documents of class `football`.\n",
    "\n",
    "From this it learns that `weather` documents are slightly more common that `football` documents, and it therefore has a slight bias towards saying a document is a `weather` document.\n",
    "\n",
    "To be more precise, the classifier has learned that:\n",
    "- The probability that a document is in class `weather` is $3/5$.  \n",
    "We say $P(\\mbox{weather})=3/5=0.6$.\n",
    "\n",
    "- The probability that a document is in class `football` is $2/5$.  \n",
    "We say $P(\\mbox{football})=2/5=0.4$.\n",
    "\n",
    "In general, if the training data contained $n_1$ documents of class `weather` and $n_2$ documents of class `football`, then \n",
    "\n",
    "$$P(\\mbox{weather})=\\frac{n_1}{n_1+n_2} \\qquad \\mbox{and} \\qquad P(\\mbox{football})=\\frac{n_2}{n_1+n_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In blank the cell below, implement a function `class_priors(training_data)` that takes a dictionary of training data  and returns a dictionary that maps the name of each class to the class prior for that class.\n",
    "\n",
    "Once you have done this, test it out on the training data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float, {'football': 0.4, 'weather': 0.6})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def class_priors(training_data):\n",
    "    docs_per_class = collections.defaultdict(int)\n",
    "    priors = collections.defaultdict(float)\n",
    "    for doc, c in training_data:\n",
    "        docs_per_class[c] += 1\n",
    "    for c in docs_per_class.keys():\n",
    "        priors[c] = docs_per_class[c]/len(training_data)\n",
    "    return priors\n",
    "class_priors(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/class_priors\n",
    "def class_priors(data):\n",
    "    doc_counts = collections.defaultdict(int)\n",
    "    priors = collections.defaultdict(float)\n",
    "    for doc,c in data:\n",
    "        doc_counts[c] += 1\n",
    "    for c in doc_counts.keys():\n",
    "        priors[c] = doc_counts[c]/len(data)\n",
    "    return priors\n",
    "\n",
    "priors = class_priors(train_data)\n",
    "priors[\"football\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional probabilities\n",
    "We now turn to the problem of how to calculate (estimates of) the probabilities such as $P(\\,d\\,|\\,\\mbox{weather}\\,)$ and $P(\\,d\\,|\\,\\mbox{football}\\,)$, for some document $d$. The problem we have is that $d$ is a document, potentially a long document, that we won't have seen in the training data.\n",
    "\n",
    "To address this, the Na√Øve Bayes model of document classification makes a major simplifying assumption.  In particular, it is assumed that the probabiity that different words occur in a document are independent of one another.\n",
    "\n",
    "For example, if $d=\\mbox{\"today is looking cloudy\"}$ then this assumption tells us that:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "P(\\,\\mbox{\"today is looking cloudy\"}\\,|\\,\\mbox{weather}\\,) &=& P(\\{\\mbox{\"today\"},\\mbox{\"is\"},\\mbox{\"looking\"},\\mbox{\"cloudy\"}\\}\\,|\\,\\mbox{weather}\\,)\\\\\n",
    "&=& P(\\,\\mbox{\"today\"}\\,|\\,\\mbox{weather}\\,)\\times P(\\mbox{\"is\"}\\,|\\,\\mbox{weather}\\,)\\times P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather}\\,)\\times P(\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather}\\,)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "For the general case, with class $c$ and document $d=\\{w_1,\\ldots,w_n\\}$, we have:\n",
    "\\begin{eqnarray*}\n",
    "P(\\,d\\,|\\,c\\,) &=& P(\\,\\{w_1,\\ldots,w_n\\}\\,|\\,c\\,)\\\\\n",
    "&=& \\prod_{i=1}^n P(\\,w_i\\,|\\,c\\,)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "The point is that it is plausible that given a reasonable amount of training data, we can make reasonable estimates of the probabilities $P(\\,\\mbox{\"today\"}\\,|\\,\\mbox{weather}\\,)$, $P(\\mbox{\"is\"}\\,|\\,\\mbox{weather}\\,)$, $P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather}\\,)$, and  $P(\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather}\\,)$. \n",
    "\n",
    "We not look at how that is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating conditional probabilities\n",
    "So we have established that we need estimates of probabilities such as: $P(\\,\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather}\\,)$, $P(\\mbox{\"is\"}\\,|\\,\\mbox{weather}\\,)$, $P(\\mbox{\"today\"}\\,|\\,\\mbox{weather}\\,)$, and $P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather}\\,)$.\n",
    "\n",
    "How can these probabilities be estimated from the training data?\n",
    "\n",
    "Look at the training data we set up above. There are 3 documents of class `weather` and within these documents there are a total of 11 tokens (8 different types).\n",
    "\n",
    "From the data we can estimate the following probabilities:\n",
    "- the probability of seeing \"today\" in a `weather` document is $\\frac{2}{11}$,  \n",
    "i.e. $P(\\mbox{\"today\"}\\,|\\,\\mbox{weather})=\\frac{2}{11}$;\n",
    "- the probability of seeing \"it\" in a `weather` document is $\\frac{2}{11}$,  \n",
    "i.e. $P(\\mbox{\"it\"}\\,|\\,\\mbox{weather})=\\frac{2}{11}$;\n",
    "- the probability of seeing \"is\" in a `weather` document is $\\frac{2}{11}$,  \n",
    "i.e. $P(\\mbox{\"is\"}\\,|\\,\\mbox{weather})=\\frac{2}{11}$;\n",
    "- the probability of seeing \"raining\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"raining\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$;\n",
    "- the probability of seeing \"looking\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$;\n",
    "- the probability of seeing \"cloudy\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$;\n",
    "- the probability of seeing \"nice\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"nice\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$; and\n",
    "- the probability of seeing \"weather\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"weather\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$\n",
    "- the probability of seeing any other word in a `weather` document is 0.\n",
    "Notice that all of these conditional probabilities sum to $1$.\n",
    "\n",
    "We can do the same thing for the `football` documents:\n",
    "- the probability of seeing \"city\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"city\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing \"looking\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing \"good\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"good\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing \"advantage\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"advantage\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing \"united\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"united\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing any other word in a `football` document is 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "We now look at how to implement the calculation of conditional probabilties.\n",
    "\n",
    "In the empty cell below, define a function `cond_probs(training_data)` that takes training data and returns a dictionary that maps the name of a class, $c$, onto a dictionary that maps each word, $w$ to the conditional probability for that word given that class, i.e. $\\log(P(w|c))$.\n",
    "\n",
    "Hint: the following line will create a dictionary of the required form:  \n",
    "`c_probs = collections.defaultdict(lambda: defaultdict(float))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09090909090909091"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cond_probs(training_data):\n",
    "    c_probs = collections.defaultdict(lambda: defaultdict(float))\n",
    "    words_per_doc = collections.defaultdict(int)\n",
    "    for doc, c in training_data:\n",
    "        words_per_doc[c] += len(doc)\n",
    "        for word in doc:\n",
    "            c_probs[c][word] += 1\n",
    "    for c in words_per_doc:\n",
    "        for word in c_probs[c]:\n",
    "             c_probs[c][word] /= words_per_doc[c]            \n",
    "    return c_probs\n",
    "    \n",
    "c_ps = cond_probs(train_data)\n",
    "\n",
    "c_ps[\"weather\"][\"cloudy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/cond_probs_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09090909090909091"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load solutions/cond_probs\n",
    "def cond_probs(training_data):\n",
    "    # c_probs will hold our conditional probabilities\n",
    "    c_probs = collections.defaultdict(lambda: collections.defaultdict(float)) \n",
    "    # docs_with_word is a mapping from a class to a mapping from a word to number of documents of that category the word appeared in \n",
    "    docs_with_word = collections.defaultdict(lambda: collections.defaultdict(int)) \n",
    "    # tot_words is a mapping from a class to the total number of words documents of that class\n",
    "    tot_words = collections.defaultdict(int)  \n",
    "    \n",
    "    # first get the counts of words in documents of a class and total word count per class\n",
    "    for doc,c in training_data:\n",
    "        for word in doc:\n",
    "            docs_with_word[c][word] += 1\n",
    "            tot_words[c] += 1\n",
    "                \n",
    "    # now compute the conditional probabilities\n",
    "    for c in docs_with_word.keys(): \n",
    "        for word in docs_with_word[c].keys():\n",
    "            c_probs[c][word] = docs_with_word[c][word] / tot_words[c]\n",
    "    return c_probs\n",
    "        \n",
    "c_ps = cond_probs(train_data)\n",
    "\n",
    "c_ps[\"weather\"][\"cloudy\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the conditional probability of a document\n",
    "\n",
    "We have created implementations of the following functions:\n",
    "- `class_priors(training_data)` that computes estimates of the class priors from training data;\n",
    "- `cond_probs(training_data)` that computes estimates of the conditional probability of a word given a class from training data\n",
    "\n",
    "Let us suppose that we have applied these functions to our training data as follows.\n",
    "\n",
    "```\n",
    "c_priors = class_priors(train_data)\n",
    "c_probs = cond_probs(train_data)\n",
    "```\n",
    "\n",
    "`c_priors` and `c_probs` define the classifier.\n",
    "\n",
    "### Exercise\n",
    "In the cell below, complete the function `classify(doc,c_priors,c_probs)`. It should return the class that the classifier assigns to the document, `doc`. \n",
    "\n",
    "In the event of a tie, the function should randomly chose one of the classes (see `random.choice`). \n",
    "\n",
    "- Write your function in a way that allows for the possibilty of any number of classes.\n",
    "- Assume that the document, `doc`, is represented as a dictionary that maps words (in the document) to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'football'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify(doc,priors,c_probs):\n",
    "    class_scores = collections.defaultdict(lambda:1)\n",
    "    for c in priors.keys():\n",
    "        class_scores[c] *= priors[c]\n",
    "        for word in doc:\n",
    "            class_scores[c] *= c_probs[c][word]\n",
    "    best = max(class_scores.values())\n",
    "    return random.choice([c for c in class_scores.keys() if class_scores[c] == best])\n",
    "    \n",
    "\n",
    "c_priors = class_priors(train_data)\n",
    "c_probs = cond_probs(train_data)\n",
    "sent = \"city looking cloudy today\"\n",
    "doc = dict([(word, True) for word in sent.split()])\n",
    "classify(doc,c_priors,c_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-03ec3e40b05b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclass_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mclass_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mc_priors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclass_priors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mc_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcond_probs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"looking cloudy today\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# %load solutions/my_NB_classify\n",
    "def classify(doc,priors,c_probs):\n",
    "    class_scores = collections.defaultdict(lambda:1)\n",
    "    for c in priors.keys():\n",
    "        class_scores[c] *= priors[c]\n",
    "        for word in doc:\n",
    "            class_scores[c] *= c_probs[c][word]\n",
    "    best_score = max(class_scores.values())\n",
    "    return random.choice([c for c in class_scores.keys() if class_scores[c]== best_score])\n",
    "\n",
    "c_priors = class_priors(data)\n",
    "c_probs = cond_probs(data)\n",
    "sent = \"looking cloudy today\"\n",
    "doc = dict([(word, True) for word in sent.split()])\n",
    "classify(doc,c_priors,c_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A problem\n",
    "There is a problem with the classifier that we have written. \n",
    "\n",
    "### Exercise\n",
    "Run the classifier on several examples to see if you can discover what the problem is. \n",
    "- You might need to run the same document through the classifier several times to see if you get different class for different runs\n",
    " - If you've implemented `classify` correctly then this happens when you have a tie.\n",
    "- Try the sentence \"city looking cloudy today\"\n",
    "\n",
    "What is going wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add one smoothing\n",
    "It will often be the case that a word appears in documents of one class, but not in any documents of another class.\n",
    "- We saw that with the word \"city\" in the document \"city looking cloudy today\".\n",
    "- While \"city\" appears in documents of class `football`, it does not appear in any documents of class `weather`. \n",
    "- Thus, the conditional probabiity $P(\\,\\mbox{city}\\,|\\,\\mbox{weather}\\,)$ is equal to zero.\n",
    "- We are **multiplying** probabilities, so the document ends up with a score of zero even though all of the other words in the document suggest that it is of class `weather`.\n",
    "\n",
    "To get around this we need to do something called **smoothing** in order to avoid zero probabilities. \n",
    "\n",
    "In particular, we will implement a version of smoothing called **add-one smoothing**. This involves adding a count of one to all of the known vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Known vocabulary\n",
    "The words that appear in documents in the training data are collectively described as the known vocabulary. These are the words that the classifier can learn something about. If the classifier is asked to classify a document that contains any words that are not in the known vocabulary then the classifier will simply ignore them.\n",
    "\n",
    "### Exercise\n",
    "In the cell below, write a function `known_vocabulary(training_data)` that takes some training and returns a set containing all of words that appear in documents in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'advantage',\n",
       " 'city',\n",
       " 'cloudy',\n",
       " 'good',\n",
       " 'is',\n",
       " 'it',\n",
       " 'looking',\n",
       " 'nice',\n",
       " 'raining',\n",
       " 'today',\n",
       " 'united',\n",
       " 'weather'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def known_vocabulary(training_data):\n",
    "    known_voc = set()\n",
    "    for doc, c in training_data:\n",
    "        for word in doc.keys():\n",
    "            known_voc.add(word)\n",
    "    return known_voc \n",
    "\n",
    "known_vocabulary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'advantage',\n",
       " 'city',\n",
       " 'cloudy',\n",
       " 'good',\n",
       " 'is',\n",
       " 'it',\n",
       " 'looking',\n",
       " 'nice',\n",
       " 'raining',\n",
       " 'today',\n",
       " 'united',\n",
       " 'weather'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load solutions/vocabulary\n",
    "def known_vocabulary(training_data):\n",
    "    vocab = set()\n",
    "    for doc,c in training_data:\n",
    "        for word in doc:\n",
    "            vocab.add(word)\n",
    "    return vocab\n",
    "            \n",
    "known_vocabulary(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing add-one smoothing\n",
    "As the name suggests, add-one smoothing involves adding counts.\n",
    "\n",
    "In particular, for each word, $w$, in the known vocabulary and each class, $c$, we add one extra count to our record of how many times $w$ appears in documents of class $c$. We are, in effect, hallucinating counts. The reason for doing this is that it means that we avoid zero probabilties.\n",
    "\n",
    "### Exercise\n",
    "In the blank cell below copy in your code for the `cond_probs` function that you wrote earlier. Then adapt this code so that it implements the add-one smoothing scheme.\n",
    "- You will find it useful to use your `known_vocabulary` function.\n",
    "- If there are $k$ words in your known vocabulary, then you will add $k$ counts for each class. \n",
    "- Therefore, when calculating conditional probabilities, you need to add $k$ to the denominator to account for these extra counts.\n",
    "\n",
    "Test out your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4782608695652174"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cond_probs(training_data):\n",
    "    c_probs = collections.defaultdict(lambda: defaultdict(float))\n",
    "    words_per_doc = collections.defaultdict(int)\n",
    "    voc = known_vocabulary(training_data)\n",
    "    for doc, c in training_data:\n",
    "        words_per_doc[c] += len(doc)\n",
    "        for word in doc:\n",
    "            c_probs[c][word] += 1\n",
    "            for word in voc:\n",
    "                c_probs[c][word] += 1\n",
    "    for c in words_per_doc:\n",
    "        words_per_doc[c] += len(voc)\n",
    "    for c in words_per_doc.keys():\n",
    "        for word in c_probs[c].keys():\n",
    "             c_probs[c][word] /= words_per_doc[c]        \n",
    "    return c_probs\n",
    "\n",
    "c_ps = cond_probs(train_data)\n",
    "\n",
    "c_ps[\"weather\"][\"city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4782608695652174"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load solutions/smoothed_cond_probs\n",
    "def cond_probs(training_data):\n",
    "    # c_probs will hold our conditional probabilities\n",
    "    c_probs = collections.defaultdict(lambda: collections.defaultdict(float)) \n",
    "    # docs_with_word is a mapping from a class to a mapping from a word to number of documents of that category the word appeared in \n",
    "    docs_with_word = collections.defaultdict(lambda: collections.defaultdict(int)) \n",
    "    # tot_words is a mapping from a class to the total number of words documents of that class\n",
    "    tot_words = collections.defaultdict(int)  \n",
    "    \n",
    "    # first get the counts of words in documents of a class and total word count per class\n",
    "    for doc,c in training_data:\n",
    "        for word in doc:\n",
    "            docs_with_word[c][word] += 1\n",
    "            tot_words[c] += 1\n",
    "\n",
    "    # next, add the add-one smoothing counts\n",
    "    known_vocab = known_vocabulary(training_data)\n",
    "    for c in docs_with_word.keys():\n",
    "        for word in known_vocab:\n",
    "            docs_with_word[c][word] += 1\n",
    "    # update tot_words to account for the additional (hallucinated) counts\n",
    "        tot_words[c] += len(known_vocab)\n",
    "    \n",
    "    # now compute the conditional probabilities\n",
    "    for c in docs_with_word.keys(): \n",
    "        for word in docs_with_word[c].keys():\n",
    "            c_probs[c][word] = docs_with_word[c][word] / tot_words[c]\n",
    "    return c_probs\n",
    "        \n",
    "cond_probs(data)\n",
    "\n",
    "c_ps[\"weather\"][\"city\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing class priors\n",
    "It is possible (though perhaps unlikely) that the training data does not contain any data for one class. In order to take care of this, we also smooth the class priors.\n",
    "\n",
    "### Exercise\n",
    "In the cell below, revise the `class_priors` function so that it adds one to the count of each class.\n",
    "\n",
    "Check that the sum of the priors for each of the classes is $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'football': 0.42857142857142855, 'weather': 0.5714285714285714}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def class_priors(training_data):\n",
    "    priors = {}\n",
    "    counts = collections.defaultdict(int)\n",
    "    for doc, c in training_data:\n",
    "        counts[c] += 1\n",
    "    for c in counts.keys():\n",
    "        counts[c] += 1\n",
    "    for c in counts.keys():\n",
    "        priors[c] = counts[c]/(len(training_data) + len(counts.items()))\n",
    "    return priors\n",
    "\n",
    "class_priors(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/smoothed_class_priors\n",
    "def class_priors(data):\n",
    "    doc_counts = collections.defaultdict(int)\n",
    "    priors = collections.defaultdict(float)\n",
    "    # first we get the document count for each class\n",
    "    for doc,c in data:\n",
    "        doc_counts[c] += 1\n",
    "    # now we add counts to achieve add-one smoothing\n",
    "    for c in doc_counts:\n",
    "        doc_counts[c] += 1\n",
    "    # now we compute the probabilities \n",
    "    # we must add len(doc_counts) to the denominator because of the add-one smoothing\n",
    "    for c in doc_counts.keys():\n",
    "        priors[c] = doc_counts[c]/(len(data)+len(doc_counts)) \n",
    "    return priors\n",
    "\n",
    "priors = class_priors(train_data)\n",
    "print(\"The prior for class 'football' is {0:.3f}.\\nThe prior for class 'weather' is {1:.3f}.\\nThe priors sum to {2:.3f}\".\n",
    "      format(priors[\"football\"],priors[\"weather\"],priors[\"football\"] + priors[\"weather\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignoring OOV words\n",
    "We now look at how to update the `classify` function that we wrote earlier so that it ignores out of vocabulary words that appear in a document being classified.\n",
    "\n",
    "### Exercise\n",
    "In the blank cell below, copy the `classify` method you wrote earlier and update it so that words not in the known vocabulary are ignored.\n",
    "- You will want to add an additional argument to the `classify` function that is a set containing the known vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weather'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify(doc,priors,c_probs, voc):\n",
    "    class_scores = collections.defaultdict(lambda:1)\n",
    "    for c in priors.keys():\n",
    "        class_scores[c] *= priors[c]\n",
    "        for word in doc:\n",
    "            if word in voc:\n",
    "                class_scores[c] *= c_probs[c][word]\n",
    "    best = max(class_scores.values())\n",
    "    return random.choice([c for c in class_scores.keys() if class_scores[c] == best])    \n",
    "\n",
    "c_priors = class_priors(train_data)\n",
    "c_probs = cond_probs(train_data)\n",
    "voc = known_vocabulary(train_data)\n",
    "sent = \"looking good today\" \n",
    "doc = dict([(word, True) for word in sent.split()])\n",
    "classify(doc,c_priors,c_probs, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/new_classify\n",
    "def classify(doc,priors,c_probs,known_vocab):\n",
    "    class_scores = collections.defaultdict(lambda:1)\n",
    "    for c in priors.keys():\n",
    "        class_scores[c] *= priors[c]\n",
    "        for word in doc:\n",
    "            if word in known_vocab:\n",
    "                class_scores[c] *= c_probs[c][word]\n",
    "    best_score = max(class_scores.values())\n",
    "    return random.choice([c for c in class_scores.keys() if class_scores[c]== best_score])\n",
    "\n",
    "c_priors = class_priors(train_data)\n",
    "c_probs = cond_probs(train_data)\n",
    "known_vocab = known_vocabulary(train_data)\n",
    "sent = \"looking really cloudy today\"\n",
    "doc = dict([(word, True) for word in sent.split()])\n",
    "classify(doc,c_priors,c_probs,known_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underflow\n",
    "\n",
    "We need to address one final problem concerning the multiplication of probabilities.\n",
    "\n",
    "Recall this equation from earlier:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "P(\\,d\\,|\\,c\\,) &=& P(\\,\\{w_1,\\ldots,w_n\\}\\,|\\,c\\,)\\\\\n",
    "&=& \\prod_{i=1}^n P(\\,w_i\\,|\\,c\\,)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "This tells us that in order to compute $P(\\,d\\,|\\,c\\,)$ for some document $d$ and class $c$, we must multiply $n$ conditional probabilities, one for each word in the document.\n",
    "\n",
    "While in our toy example, this is not an issue. However, in a more realistic settings, where we had thousands of documents, each of which contained multiple paragraphs, we would find ourselves multiplying large numbers of very small probabilities. This would lead to **underflow**.\n",
    "\n",
    "To avoid ths problem, we will add the log of probabilties. \n",
    "\n",
    "To understand why this is a reasonable thing to do let us recall a comparison from earlier:\n",
    "\n",
    "$$P(\\,d\\,|\\,\\mbox{weather}\\,)\\cdot P(\\,\\mbox{weather}\\,) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad P(\\,d\\,|\\,\\mbox{football}\\,)\\cdot P(\\,\\mbox{football}\\,)$$\n",
    "\n",
    "Our goal is to determine which of the values (on the left and right) is larger (or determine that they are equal). \n",
    "\n",
    "It should be clear that we will get exactly the same answer to this question by making the following comparsion.\n",
    "\n",
    "$$\\log(P(\\,d\\,|\\,\\mbox{weather}\\,)) + \\log(P(\\,\\mbox{weather}\\,)) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad \\log(P(\\,d\\,|\\,\\mbox{football}\\,)) + \\log(P(\\,\\mbox{football}\\,))$$\n",
    "\n",
    "Thus, rather than calculating conditional probabilities as described above, we will calculate log conditional probabilities like this:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\log(P(\\,\\mbox{\"today is looking cloudy\"}\\,|\\,\\mbox{weather}\\,)) &=& \\log(P(\\{\\mbox{\"today\"},\\mbox{\"is\"},\\mbox{\"looking\"},\\mbox{\"cloudy\"}\\}\\,|\\,\\mbox{weather}\\,))\\\\\n",
    "&=& \\log(P(\\,\\mbox{\"today\"}\\,|\\,\\mbox{weather}\\,))\\ +\\\\\n",
    "&&\\log(P(\\mbox{\"is\"}\\,|\\,\\mbox{weather}\\,))\\ +\\\\\n",
    "&&\\log(P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather}\\,))\\ + \\\\\n",
    "&&\\log(P(\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather}\\,))\n",
    "\\end{eqnarray*}\n",
    "\n",
    "For the general case, with class $c$ and document $d=\\{w_1,\\ldots,w_n\\}$, we have:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\log(P(\\,d\\,|\\,c\\,)) &=& \\log(P(\\,\\{w_1,\\ldots,w_n\\}\\,|\\,c\\,))\\\\\n",
    "&=& \\sum_{i=1}^n \\log(P(\\,w_i\\,|\\,c\\,))\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the blank cell below, make a copy of the cell containing the definition of `classify`.\n",
    "\n",
    "Adapt the code so that it adds logs of probabilties rather than multiplies probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weather'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify(doc, priors, c_probs, voc):\n",
    "    class_scores = collections.defaultdict(lambda:1)\n",
    "    for c in priors.keys():\n",
    "        class_scores[c] += math.log(priors[c])\n",
    "        for word in doc:\n",
    "            if word in voc:\n",
    "                class_scores[c] += math.log(c_probs[c][word])\n",
    "    best = max(class_scores.values())\n",
    "    return random.choice([c for c in class_scores.keys() if class_scores[c] == best])    \n",
    "\n",
    "c_priors = class_priors(train_data)\n",
    "c_probs = cond_probs(train_data)\n",
    "voc = known_vocabulary(train_data)\n",
    "sent = \"looking good today\" \n",
    "doc = dict([(word, True) for word in sent.split()])\n",
    "classify(doc,c_priors,c_probs, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weather'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load solutions/classify_final\n",
    "def classify(doc,priors,c_probs,known_vocab):\n",
    "   class_scores = collections.defaultdict(lambda:0)\n",
    "   for c in priors.keys():\n",
    "       class_scores[c] += math.log(priors[c])\n",
    "       for word in doc:\n",
    "           if word in known_vocab:\n",
    "               class_scores[c] += math.log(c_probs[c][word])\n",
    "   best_score = max(class_scores.values())\n",
    "   return random.choice([c for c in class_scores.keys() if class_scores[c]== best_score])\n",
    "\n",
    "c_priors = class_priors(train_data)\n",
    "c_probs = cond_probs(train_data)\n",
    "known_vocab = known_vocabulary(train_data)\n",
    "sent = \"city looking cloudy today\"\n",
    "doc = dict([(word, True) for word in sent.split()])\n",
    "classify(doc,c_priors,c_probs,known_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a Na√Øve Bayes classifier on test data\n",
    "We are now ready to run our Na√Øve Bayes classifier on a set of test data. When we do this we want to return the accuracy of the classifier on that data, where accuracy is calculated as follows:\n",
    "\n",
    "$$\\frac{\\mbox{number of test documents that the classifier classifiers correctly}}\n",
    "{\\mbox{total number of test documents}}$$\n",
    "\n",
    "In order to compute this accuracy score, we need to give the classifier **labelled** test data.\n",
    "- This will be in the same format as the training data.\n",
    "\n",
    "### Exercise\n",
    "In the cell below, we set up 5 test documents in the class `weather` and 5 documents in the class `football`.\n",
    "\n",
    "Run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_sents_train = [\n",
    "    \"today it is raining\",\n",
    "    \"looking cloudy today\",\n",
    "    \"it is nice weather\",\n",
    "]\n",
    "\n",
    "football_sents_train = [\n",
    "    \"city looking good\",\n",
    "    \"advantage united\",\n",
    "]\n",
    "\n",
    "weather_data_train = [(dict([(word, True) for word in sent.split()]), \"weather\") for sent in weather_sents_train] \n",
    "football_data_train = [(dict([(word, True) for word in sent.split()]), \"football\") for sent in football_sents_train]\n",
    "train_data = weather_data_train + football_data_train\n",
    "\n",
    "weather_sents_test = [\n",
    "    \"the weather today is nice\",\n",
    "    \"it is raining cats and dogs\",\n",
    "    \"the weather here is wet\",\n",
    "    \"goal it was hot today\",\n",
    "    \"rain due tomorrow\",\n",
    "    \"shots raining down on the keeper\"\n",
    "]\n",
    "\n",
    "football_sents_test = [\n",
    "    \"what a great goal that was\",\n",
    "    \"poor defending by the city center back\",\n",
    "    \"wow he missed a sitter\",\n",
    "    \"united are a shambles\",\n",
    "    \"shots raining down on the keeper\",\n",
    "]\n",
    "\n",
    "weather_data_test = [(dict([(word, True) for word in sent.split()]), \"weather\") for sent in weather_sents_test] \n",
    "football_data_test = [(dict([(word, True) for word in sent.split()]), \"football\") for sent in football_sents_test]\n",
    "test_data = weather_data_test + football_data_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the cell below Implement a `NB_evaluate` function that returns the accuracy of a classifier on a set of labelled test data.\n",
    "`NB_evaluate` should make use of the `classify` function, and take the following arguments:\n",
    "- the test data\n",
    "- the class priors\n",
    "- the conditional probabilities\n",
    "- the known vocabulary (though this is redundant since it could be computed from the conditional probabilities)\n",
    "\n",
    "`NB_evaluate` should return the accuracy of the classifier on the test data.\n",
    "\n",
    "Try out your `NB_evaluate` function on the test data in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5454545454545454"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NB_evaluate(test_data, priors, c_probs, voc):\n",
    "    correct = 0\n",
    "    for doc, c in test_data:\n",
    "        pred_class = classify(doc, priors, c_probs, voc)\n",
    "        if pred_class == c:\n",
    "            correct += 1            \n",
    "    return correct/len(test_data)\n",
    "\n",
    "priors = class_priors(test_data)\n",
    "c_probs = cond_probs(test_data)\n",
    "voc = known_vocabulary(test_data)\n",
    "NB_evaluate(test_data, priors, c_probs, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5454545454545454"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load solutions/NB_evaluate\n",
    "def NB_evaluate(test_data,priors,c_probs,known_vocab):\n",
    "    num_correct = 0\n",
    "    for doc,c in test_data:\n",
    "        predicted_class = classify(doc,priors,c_probs,known_vocab)\n",
    "        if predicted_class == c:\n",
    "            num_correct += 1\n",
    "    return num_correct/len(test_data)\n",
    "\n",
    "NB_evaluate(test_data,priors,c_probs,known_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Try out the classifier on different training and test examples.\n",
    "\n",
    "Before running each test try to anticipate what you think the outcome will be.\n",
    "- What happens when none of the words in the test document are in the known vocabulary?\n",
    "- Can you set up data so that there is a tie between the classes, and the classifier randomly chooses a class. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
